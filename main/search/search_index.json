{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>TODO</p>"},{"location":"#motivation","title":"Motivation","text":"<p>TODO</p>"},{"location":"#api-stability","title":"API stability","text":"<p> While <code>arkas</code> is in development stage, no API is guaranteed to be stable from one release to the next. In fact, it is very likely that the API will change multiple times before a stable 1.0.0 release. In practice, this means that upgrading <code>arkas</code> to a new version will possibly break any code that was using the old version of <code>arkas</code>.</p>"},{"location":"#license","title":"License","text":"<p><code>arkas</code> is licensed under BSD 3-Clause \"New\" or \"Revised\" license available in LICENSE file.</p>"},{"location":"get_started/","title":"Get Started","text":"<p>It is highly recommended to install in a virtual environment to keep your system in order.</p>"},{"location":"get_started/#installing-with-pip-recommended","title":"Installing with <code>pip</code> (recommended)","text":"<p>The following command installs the latest version of the library:</p> <pre><code>pip install arkas\n</code></pre> <p>To make the package as slim as possible, only the packages required to use <code>arkas</code> are installed. It is possible to install all the optional dependencies by running the following command:</p> <pre><code>pip install 'arkas[all]'\n</code></pre> <p>This command also installed NumPy and PyTorch. It is also possible to install the optional packages manually or to select the packages to install. In the following example, only NumPy is installed:</p> <pre><code>pip install arkas numpy\n</code></pre>"},{"location":"get_started/#installing-from-source","title":"Installing from source","text":"<p>To install <code>arkas</code> from source, you can follow the steps below. First, you will need to install <code>poetry</code>. <code>poetry</code> is used to manage and install the dependencies. If <code>poetry</code> is already installed on your machine, you can skip this step. There are several ways to install <code>poetry</code> so you can use the one that you prefer. You can check the <code>poetry</code> installation by running the following command:</p> <pre><code>poetry --version\n</code></pre> <p>Then, you can clone the git repository:</p> <pre><code>git clone git@github.com:durandtibo/arkas.git\n</code></pre> <p>It is recommended to create a Python 3.8+ virtual environment. This step is optional so you can skip it. To create a virtual environment, you can use the following command:</p> <pre><code>make conda\n</code></pre> <p>It automatically creates a conda virtual environment. When the virtual environment is created, you can activate it with the following command:</p> <pre><code>conda activate arkas\n</code></pre> <p>This example uses <code>conda</code> to create a virtual environment, but you can use other tools or configurations. Then, you should install the required package to use <code>arkas</code> with the following command:</p> <pre><code>make install\n</code></pre> <p>This command will install all the required packages. You can also use this command to update the required packages. This command will check if there is a more recent package available and will install it. Finally, you can test the installation with the following command:</p> <pre><code>make unit-test-cov\n</code></pre>"},{"location":"refs/evaluator/","title":"arkas.evaluator","text":""},{"location":"refs/evaluator/#arkas.evaluator","title":"arkas.evaluator","text":"<p>Contain evaluators.</p>"},{"location":"refs/evaluator/#arkas.evaluator.AccuracyEvaluator","title":"arkas.evaluator.AccuracyEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[AccuracyResult]</code></p> <p>Implement the accuracy evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The column name of the predicted labels.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import AccuracyEvaluator\n&gt;&gt;&gt; evaluator = AccuracyEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nAccuracyEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [3, 2, 0, 1, 0, 1], \"target\": [3, 2, 0, 1, 0, 1]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nAccuracyResult(y_true=(6,), y_pred=(6,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.AveragePrecisionEvaluator","title":"arkas.evaluator.AveragePrecisionEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[AveragePrecisionResult]</code></p> <p>Implement the average precision evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_score</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, <code>'multilabel'</code>, and <code>'auto'</code>. If <code>'auto'</code>, it tries to automatically find the label type from the arrays' shape.</p> <code>'auto'</code> <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_score</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import AveragePrecisionEvaluator\n&gt;&gt;&gt; evaluator = AveragePrecisionEvaluator(y_true=\"target\", y_score=\"pred\")\n&gt;&gt;&gt; evaluator\nAveragePrecisionEvaluator(y_true=target, y_score=pred, label_type=auto, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [2, -1, 0, 3, 1], \"target\": [1, 0, 0, 1, 1]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nAveragePrecisionResult(y_true=(5,), y_score=(5,), label_type=binary)\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.BalancedAccuracyEvaluator","title":"arkas.evaluator.BalancedAccuracyEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[BalancedAccuracyResult]</code></p> <p>Implement the accuracy evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import BalancedAccuracyEvaluator\n&gt;&gt;&gt; evaluator = BalancedAccuracyEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nBalancedAccuracyEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [3, 2, 0, 1, 0, 1], \"target\": [3, 2, 0, 1, 0, 1]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nBalancedAccuracyResult(y_true=(6,), y_pred=(6,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.BaseEvaluator","title":"arkas.evaluator.BaseEvaluator","text":"<p>               Bases: <code>ABC</code></p> <p>Define the base class to evaluate a DataFrame.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import AccuracyEvaluator\n&gt;&gt;&gt; evaluator = AccuracyEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nAccuracyEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [3, 2, 0, 1, 0, 1], \"target\": [3, 2, 0, 1, 0, 1]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nAccuracyResult(y_true=(6,), y_pred=(6,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.BaseEvaluator.evaluate","title":"arkas.evaluator.BaseEvaluator.evaluate","text":"<pre><code>evaluate(data: DataFrame, lazy: bool = True) -&gt; BaseResult\n</code></pre> <p>Evaluate the result.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data to evaluate.</p> required <code>lazy</code> <code>bool</code> <p>If <code>True</code>, it forces the computation of the result, otherwise it returns a result object that delays the evaluation of the result.</p> <code>True</code> <p>Returns:</p> Type Description <code>BaseResult</code> <p>The generated result.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import AccuracyEvaluator\n&gt;&gt;&gt; evaluator = AccuracyEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [3, 2, 0, 1, 0, 1], \"target\": [3, 2, 0, 1, 0, 1]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nAccuracyResult(y_true=(6,), y_pred=(6,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.BaseLazyEvaluator","title":"arkas.evaluator.BaseLazyEvaluator","text":"<p>               Bases: <code>BaseEvaluator</code>, <code>Generic[T]</code></p> <p>Define the base class to evaluate the result lazily.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import AccuracyEvaluator\n&gt;&gt;&gt; evaluator = AccuracyEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nAccuracyEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [3, 2, 0, 1, 0, 1], \"target\": [3, 2, 0, 1, 0, 1]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nAccuracyResult(y_true=(6,), y_pred=(6,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.BinaryAveragePrecisionEvaluator","title":"arkas.evaluator.BinaryAveragePrecisionEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[BinaryAveragePrecisionResult]</code></p> <p>Implement the average precision evaluator for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_score</code> <code>str</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import BinaryAveragePrecisionEvaluator\n&gt;&gt;&gt; evaluator = BinaryAveragePrecisionEvaluator(y_true=\"target\", y_score=\"pred\")\n&gt;&gt;&gt; evaluator\nBinaryAveragePrecisionEvaluator(y_true=target, y_score=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [2, -1, 0, 3, 1], \"target\": [1, 0, 0, 1, 1]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nBinaryAveragePrecisionResult(y_true=(5,), y_score=(5,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.BinaryClassificationEvaluator","title":"arkas.evaluator.BinaryClassificationEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[BinaryClassificationResult]</code></p> <p>Implement the average precision evaluator for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>y_score</code> <code>str | None</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions.</p> <code>None</code> <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import BinaryClassificationEvaluator\n&gt;&gt;&gt; evaluator = BinaryClassificationEvaluator(\n...     y_true=\"target\", y_pred=\"pred\", y_score=\"score\"\n... )\n&gt;&gt;&gt; evaluator\nBinaryClassificationEvaluator(y_true=target, y_pred=pred, y_score=score, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame(\n...     {\n...         \"pred\": [1, 0, 0, 1, 1],\n...         \"score\": [2, -1, 0, 3, 1],\n...         \"target\": [1, 0, 0, 1, 1],\n...     }\n... )\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nBinaryClassificationResult(y_true=(5,), y_pred=(5,), y_score=(5,), betas=(1,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.BinaryConfusionMatrixEvaluator","title":"arkas.evaluator.BinaryConfusionMatrixEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[BinaryConfusionMatrixResult]</code></p> <p>Implement the confusion matrix evaluator for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import BinaryConfusionMatrixEvaluator\n&gt;&gt;&gt; evaluator = BinaryConfusionMatrixEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nBinaryConfusionMatrixEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [1, 0, 0, 1, 1], \"target\": [1, 0, 0, 1, 1]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nBinaryConfusionMatrixResult(y_true=(5,), y_pred=(5,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.BinaryFbetaScoreEvaluator","title":"arkas.evaluator.BinaryFbetaScoreEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[BinaryFbetaScoreResult]</code></p> <p>Implement the F-beta evaluator for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>betas</code> <code>Sequence[float]</code> <p>The betas used to compute the F-beta scores.</p> <code>(1)</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import BinaryFbetaScoreEvaluator\n&gt;&gt;&gt; evaluator = BinaryFbetaScoreEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nBinaryFbetaScoreEvaluator(y_true=target, y_pred=pred, betas=(1,), drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [1, 0, 0, 1, 1], \"target\": [1, 0, 0, 1, 1]})\n&gt;&gt;&gt; result = evaluator.evaluate(\n...     data=pl.DataFrame({\"pred\": [1, 0, 0, 1, 1], \"target\": [1, 0, 0, 1, 1]})\n... )\n&gt;&gt;&gt; result\nBinaryFbetaScoreResult(y_true=(5,), y_pred=(5,), betas=(1,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.BinaryJaccardEvaluator","title":"arkas.evaluator.BinaryJaccardEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[BinaryJaccardResult]</code></p> <p>Implement the Jaccard evaluator for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import BinaryJaccardEvaluator\n&gt;&gt;&gt; evaluator = BinaryJaccardEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nBinaryJaccardEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [1, 0, 0, 1, 1], \"target\": [1, 0, 0, 1, 1]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nBinaryJaccardResult(y_true=(5,), y_pred=(5,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.BinaryPrecisionEvaluator","title":"arkas.evaluator.BinaryPrecisionEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[BinaryPrecisionResult]</code></p> <p>Implement the precision evaluator for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import BinaryPrecisionEvaluator\n&gt;&gt;&gt; evaluator = BinaryPrecisionEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nBinaryPrecisionEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [1, 0, 0, 1, 1], \"target\": [1, 0, 0, 1, 1]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nBinaryPrecisionResult(y_true=(5,), y_pred=(5,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.BinaryRecallEvaluator","title":"arkas.evaluator.BinaryRecallEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[BinaryRecallResult]</code></p> <p>Implement the recall evaluator for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import BinaryRecallEvaluator\n&gt;&gt;&gt; evaluator = BinaryRecallEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nBinaryRecallEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [1, 0, 0, 1, 1], \"target\": [1, 0, 0, 1, 1]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nBinaryRecallResult(y_true=(5,), y_pred=(5,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.BinaryRocAucEvaluator","title":"arkas.evaluator.BinaryRocAucEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[BinaryRocAucResult]</code></p> <p>Implement the Area Under the Receiver Operating Characteristic Curve (ROC AUC) evaluator for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_score</code> <code>str</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import BinaryRocAucEvaluator\n&gt;&gt;&gt; evaluator = BinaryRocAucEvaluator(y_true=\"target\", y_score=\"pred\")\n&gt;&gt;&gt; evaluator\nBinaryRocAucEvaluator(y_true=target, y_score=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [2, -1, 0, 3, 1], \"target\": [1, 0, 0, 1, 1]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nBinaryRocAucResult(y_true=(5,), y_score=(5,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MappingEvaluator","title":"arkas.evaluator.MappingEvaluator","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Implement an evaluator that sequentially evaluates a mapping of evaluators.</p> <p>Parameters:</p> Name Type Description Default <code>evaluators</code> <code>Mapping[Hashable, BaseEvaluator]</code> <p>The mapping of evaluators to evaluate.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import (\n...     MappingEvaluator,\n...     BinaryPrecisionEvaluator,\n...     BinaryRecallEvaluator,\n... )\n&gt;&gt;&gt; evaluator = MappingEvaluator(\n...     {\n...         \"precision\": BinaryPrecisionEvaluator(y_true=\"target\", y_pred=\"pred\"),\n...         \"recall\": BinaryRecallEvaluator(y_true=\"target\", y_pred=\"pred\"),\n...     }\n... )\n&gt;&gt;&gt; evaluator\nMappingEvaluator(\n  (precision): BinaryPrecisionEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n  (recall): BinaryRecallEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [1, 0, 0, 1, 1], \"target\": [1, 0, 0, 1, 1]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMappingResult(count=2)\n&gt;&gt;&gt; result = evaluator.evaluate(data, lazy=False)\n&gt;&gt;&gt; result\nResult(metrics=2, figures=2)\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MeanAbsoluteErrorEvaluator","title":"arkas.evaluator.MeanAbsoluteErrorEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[MeanAbsoluteErrorResult]</code></p> <p>Implement the mean absolute error (MAE) evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target values.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted values.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import MeanAbsoluteErrorEvaluator\n&gt;&gt;&gt; evaluator = MeanAbsoluteErrorEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nMeanAbsoluteErrorEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [1, 2, 3, 4, 5], \"target\": [1, 2, 3, 4, 5]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMeanAbsoluteErrorResult(y_true=(5,), y_pred=(5,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MulticlassAveragePrecisionEvaluator","title":"arkas.evaluator.MulticlassAveragePrecisionEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[MulticlassAveragePrecisionResult]</code></p> <p>Implement the average precision evaluator for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_score</code> <code>str</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_score</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import MulticlassAveragePrecisionEvaluator\n&gt;&gt;&gt; evaluator = MulticlassAveragePrecisionEvaluator(y_true=\"target\", y_score=\"pred\")\n&gt;&gt;&gt; evaluator\nMulticlassAveragePrecisionEvaluator(y_true=target, y_score=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame(\n...     {\n...         \"pred\": [\n...             [0.7, 0.2, 0.1],\n...             [0.4, 0.3, 0.3],\n...             [0.1, 0.8, 0.1],\n...             [0.2, 0.5, 0.3],\n...             [0.3, 0.3, 0.4],\n...             [0.1, 0.2, 0.7],\n...         ],\n...         \"target\": [0, 0, 1, 1, 2, 2],\n...     },\n...     schema={\"pred\": pl.Array(pl.Float64, 3), \"target\": pl.Int64},\n... )\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMulticlassAveragePrecisionResult(y_true=(6,), y_score=(6, 3))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MulticlassConfusionMatrixEvaluator","title":"arkas.evaluator.MulticlassConfusionMatrixEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[MulticlassConfusionMatrixResult]</code></p> <p>Implement the confusion matrix evaluator for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import MulticlassConfusionMatrixEvaluator\n&gt;&gt;&gt; evaluator = MulticlassConfusionMatrixEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nMulticlassConfusionMatrixEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [0, 0, 1, 1, 2, 2], \"target\": [0, 0, 1, 1, 2, 2]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMulticlassConfusionMatrixResult(y_true=(6,), y_pred=(6,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MulticlassFbetaScoreEvaluator","title":"arkas.evaluator.MulticlassFbetaScoreEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[MulticlassFbetaScoreResult]</code></p> <p>Implement the F-beta evaluator for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>betas</code> <code>Sequence[float]</code> <p>The betas used to compute the F-beta scores.</p> <code>(1)</code> <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import MulticlassFbetaScoreEvaluator\n&gt;&gt;&gt; evaluator = MulticlassFbetaScoreEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nMulticlassFbetaScoreEvaluator(y_true=target, y_pred=pred, betas=(1,), drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [0, 0, 1, 1, 2, 2], \"target\": [0, 0, 1, 1, 2, 2]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMulticlassFbetaScoreResult(y_true=(6,), y_pred=(6,), betas=(1,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MulticlassJaccardEvaluator","title":"arkas.evaluator.MulticlassJaccardEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[MulticlassJaccardResult]</code></p> <p>Implement the Jaccard evaluator for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import MulticlassJaccardEvaluator\n&gt;&gt;&gt; evaluator = MulticlassJaccardEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nMulticlassJaccardEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [0, 0, 1, 1, 2, 2], \"target\": [0, 0, 1, 1, 2, 2]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMulticlassJaccardResult(y_true=(6,), y_pred=(6,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MulticlassPrecisionEvaluator","title":"arkas.evaluator.MulticlassPrecisionEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[MulticlassPrecisionResult]</code></p> <p>Implement the precision evaluator for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import MulticlassPrecisionEvaluator\n&gt;&gt;&gt; evaluator = MulticlassPrecisionEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nMulticlassPrecisionEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [0, 0, 1, 1, 2, 2], \"target\": [0, 0, 1, 1, 2, 2]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMulticlassPrecisionResult(y_true=(6,), y_pred=(6,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MulticlassRecallEvaluator","title":"arkas.evaluator.MulticlassRecallEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[MulticlassRecallResult]</code></p> <p>Implement the recall evaluator for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import MulticlassRecallEvaluator\n&gt;&gt;&gt; evaluator = MulticlassRecallEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nMulticlassRecallEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [0, 0, 1, 1, 2, 2], \"target\": [0, 0, 1, 1, 2, 2]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMulticlassRecallResult(y_true=(6,), y_pred=(6,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MulticlassRocAucEvaluator","title":"arkas.evaluator.MulticlassRocAucEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[MulticlassRocAucResult]</code></p> <p>Implement the Area Under the Receiver Operating Characteristic Curve (ROC AUC) evaluator for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_score</code> <code>str</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_score</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import MulticlassRocAucEvaluator\n&gt;&gt;&gt; evaluator = MulticlassRocAucEvaluator(y_true=\"target\", y_score=\"pred\")\n&gt;&gt;&gt; evaluator\nMulticlassRocAucEvaluator(y_true=target, y_score=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame(\n...     {\n...         \"pred\": [\n...             [0.7, 0.2, 0.1],\n...             [0.4, 0.3, 0.3],\n...             [0.1, 0.8, 0.1],\n...             [0.2, 0.5, 0.3],\n...             [0.3, 0.3, 0.4],\n...             [0.1, 0.2, 0.7],\n...         ],\n...         \"target\": [0, 0, 1, 1, 2, 2],\n...     },\n...     schema={\"pred\": pl.Array(pl.Float64, 3), \"target\": pl.Int64},\n... )\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMulticlassRocAucResult(y_true=(6,), y_score=(6, 3))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MultilabelAveragePrecisionEvaluator","title":"arkas.evaluator.MultilabelAveragePrecisionEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[MultilabelAveragePrecisionResult]</code></p> <p>Implement the average precision evaluator for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_score</code> <code>str</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_score</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import MultilabelAveragePrecisionEvaluator\n&gt;&gt;&gt; evaluator = MultilabelAveragePrecisionEvaluator(y_true=\"target\", y_score=\"pred\")\n&gt;&gt;&gt; evaluator\nMultilabelAveragePrecisionEvaluator(y_true=target, y_score=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame(\n...     {\n...         \"pred\": [[2, -1, 1], [-1, 1, -2], [0, 2, -3], [3, -2, 4], [1, -3, 5]],\n...         \"target\": [[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]],\n...     },\n...     schema={\"pred\": pl.Array(pl.Int64, 3), \"target\": pl.Array(pl.Int64, 3)},\n... )\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMultilabelAveragePrecisionResult(y_true=(5, 3), y_score=(5, 3))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MultilabelConfusionMatrixEvaluator","title":"arkas.evaluator.MultilabelConfusionMatrixEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[MultilabelConfusionMatrixResult]</code></p> <p>Implement the confusion matrix evaluator for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import MultilabelConfusionMatrixEvaluator\n&gt;&gt;&gt; evaluator = MultilabelConfusionMatrixEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nMultilabelConfusionMatrixEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame(\n...     {\n...         \"pred\": [[1, 0, 0], [0, 1, 1], [0, 1, 1], [1, 0, 0], [1, 0, 0]],\n...         \"target\": [[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]],\n...     },\n...     schema={\"pred\": pl.Array(pl.Int64, 3), \"target\": pl.Array(pl.Int64, 3)},\n... )\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMultilabelConfusionMatrixResult(y_true=(5, 3), y_pred=(5, 3))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MultilabelFbetaScoreEvaluator","title":"arkas.evaluator.MultilabelFbetaScoreEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[MultilabelFbetaScoreResult]</code></p> <p>Implement the F-beta evaluator for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>betas</code> <code>Sequence[float]</code> <p>The betas used to compute the F-beta scores.</p> <code>(1)</code> <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import MultilabelFbetaScoreEvaluator\n&gt;&gt;&gt; evaluator = MultilabelFbetaScoreEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nMultilabelFbetaScoreEvaluator(y_true=target, y_pred=pred, betas=(1,), drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame(\n...     {\n...         \"pred\": [[1, 0, 0], [0, 1, 1], [0, 1, 1], [1, 0, 0], [1, 0, 0]],\n...         \"target\": [[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]],\n...     },\n...     schema={\"pred\": pl.Array(pl.Int64, 3), \"target\": pl.Array(pl.Int64, 3)},\n... )\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMultilabelFbetaScoreResult(y_true=(5, 3), y_pred=(5, 3), betas=(1,))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MultilabelJaccardEvaluator","title":"arkas.evaluator.MultilabelJaccardEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[MultilabelJaccardResult]</code></p> <p>Implement the Jaccard evaluator for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import MultilabelJaccardEvaluator\n&gt;&gt;&gt; evaluator = MultilabelJaccardEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nMultilabelJaccardEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame(\n...     {\n...         \"pred\": [[1, 0, 0], [0, 1, 1], [0, 1, 1], [1, 0, 0], [1, 0, 0]],\n...         \"target\": [[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]],\n...     },\n...     schema={\"pred\": pl.Array(pl.Int64, 3), \"target\": pl.Array(pl.Int64, 3)},\n... )\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMultilabelJaccardResult(y_true=(5, 3), y_pred=(5, 3))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MultilabelPrecisionEvaluator","title":"arkas.evaluator.MultilabelPrecisionEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[MultilabelPrecisionResult]</code></p> <p>Implement the precision evaluator for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import MultilabelPrecisionEvaluator\n&gt;&gt;&gt; evaluator = MultilabelPrecisionEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nMultilabelPrecisionEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame(\n...     {\n...         \"pred\": [[1, 0, 0], [0, 1, 1], [0, 1, 1], [1, 0, 0], [1, 0, 0]],\n...         \"target\": [[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]],\n...     },\n...     schema={\"pred\": pl.Array(pl.Int64, 3), \"target\": pl.Array(pl.Int64, 3)},\n... )\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMultilabelPrecisionResult(y_true=(5, 3), y_pred=(5, 3))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MultilabelRecallEvaluator","title":"arkas.evaluator.MultilabelRecallEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[MultilabelRecallResult]</code></p> <p>Implement the recall evaluator for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_pred</code> <code>str</code> <p>The key or column name of the predicted labels.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_pred</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import MultilabelRecallEvaluator\n&gt;&gt;&gt; evaluator = MultilabelRecallEvaluator(y_true=\"target\", y_pred=\"pred\")\n&gt;&gt;&gt; evaluator\nMultilabelRecallEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame(\n...     {\n...         \"pred\": [[1, 0, 0], [0, 1, 1], [0, 1, 1], [1, 0, 0], [1, 0, 0]],\n...         \"target\": [[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]],\n...     },\n...     schema={\"pred\": pl.Array(pl.Int64, 3), \"target\": pl.Array(pl.Int64, 3)},\n... )\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMultilabelRecallResult(y_true=(5, 3), y_pred=(5, 3))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.MultilabelRocAucEvaluator","title":"arkas.evaluator.MultilabelRocAucEvaluator","text":"<p>               Bases: <code>BaseLazyEvaluator[MultilabelRocAucResult]</code></p> <p>Implement the Area Under the Receiver Operating Characteristic Curve (ROC AUC) evaluator for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>str</code> <p>The key or column name of the ground truth target labels.</p> required <code>y_score</code> <code>str</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions.</p> required <code>drop_nulls</code> <code>bool</code> <p>If <code>True</code>, the rows with null values in <code>y_true</code> or <code>y_score</code> columns are dropped.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import MultilabelRocAucEvaluator\n&gt;&gt;&gt; evaluator = MultilabelRocAucEvaluator(y_true=\"target\", y_score=\"pred\")\n&gt;&gt;&gt; evaluator\nMultilabelRocAucEvaluator(y_true=target, y_score=pred, drop_nulls=True)\n&gt;&gt;&gt; data = pl.DataFrame(\n...     {\n...         \"pred\": [[2, -1, 1], [-1, 1, -2], [0, 2, -3], [3, -2, 4], [1, -3, 5]],\n...         \"target\": [[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]],\n...     },\n...     schema={\"pred\": pl.Array(pl.Int64, 3), \"target\": pl.Array(pl.Int64, 3)},\n... )\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nMultilabelRocAucResult(y_true=(5, 3), y_score=(5, 3))\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.SequentialEvaluator","title":"arkas.evaluator.SequentialEvaluator","text":"<p>               Bases: <code>BaseEvaluator</code></p> <p>Implement an evaluator that sequentially evaluates several evaluators.</p> <p>Parameters:</p> Name Type Description Default <code>evaluators</code> <code>Sequence[BaseEvaluator | dict]</code> <p>The sequence of evaluators to evaluate.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.evaluator import (\n...     SequentialEvaluator,\n...     BinaryPrecisionEvaluator,\n...     BinaryRecallEvaluator,\n... )\n&gt;&gt;&gt; evaluator = SequentialEvaluator(\n...     [\n...         BinaryPrecisionEvaluator(y_true=\"target\", y_pred=\"pred\"),\n...         BinaryRecallEvaluator(y_true=\"target\", y_pred=\"pred\"),\n...     ]\n... )\n&gt;&gt;&gt; evaluator\nSequentialEvaluator(\n  (0): BinaryPrecisionEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n  (1): BinaryRecallEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n)\n&gt;&gt;&gt; data = pl.DataFrame({\"pred\": [1, 0, 0, 1, 1], \"target\": [1, 0, 0, 1, 1]})\n&gt;&gt;&gt; result = evaluator.evaluate(data)\n&gt;&gt;&gt; result\nSequentialResult(count=2)\n&gt;&gt;&gt; result = evaluator.evaluate(data, lazy=False)\n&gt;&gt;&gt; result\nResult(metrics=3, figures=1)\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.is_evaluator_config","title":"arkas.evaluator.is_evaluator_config","text":"<pre><code>is_evaluator_config(config: dict) -&gt; bool\n</code></pre> <p>Indicate if the input configuration is a configuration for a <code>BaseEvaluator</code>.</p> <p>This function only checks if the value of the key  <code>_target_</code> is valid. It does not check the other values. If <code>_target_</code> indicates a function, the returned type hint is used to check the class.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>The configuration to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the input configuration is a configuration for a <code>BaseEvaluator</code> object.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.evaluator import is_evaluator_config\n&gt;&gt;&gt; is_evaluator_config({\"_target_\": \"arkas.evaluator.AccuracyEvaluator\"})\nTrue\n</code></pre>"},{"location":"refs/evaluator/#arkas.evaluator.setup_evaluator","title":"arkas.evaluator.setup_evaluator","text":"<pre><code>setup_evaluator(\n    evaluator: BaseEvaluator | dict,\n) -&gt; BaseEvaluator\n</code></pre> <p>Set up an evaluator.</p> <p>The evaluator is instantiated from its configuration by using the <code>BaseEvaluator</code> factory function.</p> <p>Parameters:</p> Name Type Description Default <code>evaluator</code> <code>BaseEvaluator | dict</code> <p>An evaluator or its configuration.</p> required <p>Returns:</p> Type Description <code>BaseEvaluator</code> <p>An instantiated evaluator.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.evaluator import setup_evaluator\n&gt;&gt;&gt; evaluator = setup_evaluator(\n...     {\n...         \"_target_\": \"arkas.evaluator.AccuracyEvaluator\",\n...         \"y_true\": \"target\",\n...         \"y_pred\": \"pred\",\n...     }\n... )\n&gt;&gt;&gt; evaluator\nAccuracyEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n</code></pre>"},{"location":"refs/metric/","title":"arkas.metric","text":""},{"location":"refs/metric/#arkas.metric","title":"arkas.metric","text":"<p>Contain functions to compute metrics.</p>"},{"location":"refs/metric/#arkas.metric.accuracy","title":"arkas.metric.accuracy","text":"<pre><code>accuracy(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the accuracy metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import accuracy\n&gt;&gt;&gt; accuracy(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'accuracy': 1.0, 'count_correct': 5, 'count_incorrect': 0, 'count': 5, 'error': 0.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.average_precision","title":"arkas.metric.average_precision","text":"<pre><code>average_precision(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    label_type: str = \"auto\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the average precision metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, and <code>'multilabel'</code>. If <code>'binary'</code> or <code>'multilabel'</code>, <code>y_true</code> values  must be <code>0</code> and <code>1</code>.</p> <code>'auto'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import average_precision\n&gt;&gt;&gt; # auto\n&gt;&gt;&gt; metrics = average_precision(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_score=np.array([2, -1, 0, 3, 1])\n... )\n&gt;&gt;&gt; metrics\n{'average_precision': 1.0, 'count': 5}\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; metrics = average_precision(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_score=np.array([2, -1, 0, 3, 1]),\n...     label_type=\"binary\",\n... )\n&gt;&gt;&gt; metrics\n{'average_precision': 1.0, 'count': 5}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; metrics = average_precision(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_score=np.array(\n...         [\n...             [0.7, 0.2, 0.1],\n...             [0.4, 0.3, 0.3],\n...             [0.1, 0.8, 0.1],\n...             [0.2, 0.3, 0.5],\n...             [0.4, 0.4, 0.2],\n...             [0.1, 0.2, 0.7],\n...         ]\n...     ),\n...     label_type=\"multiclass\",\n... )\n&gt;&gt;&gt; metrics\n{'average_precision': array([0.833..., 0.75 , 0.75 ]),\n 'count': 6,\n 'macro_average_precision': 0.777...,\n 'micro_average_precision': 0.75,\n 'weighted_average_precision': 0.777...}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; metrics = average_precision(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_score=np.array([[2, -1, -1], [-1, 1, 2], [0, 2, 3], [3, -2, -4], [1, -3, -5]]),\n...     label_type=\"multilabel\",\n... )\n&gt;&gt;&gt; metrics\n{'average_precision': array([1. , 1. , 0.477...]),\n 'count': 5,\n 'macro_average_precision': 0.825...,\n 'micro_average_precision': 0.588...,\n 'weighted_average_precision': 0.804...}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.balanced_accuracy","title":"arkas.metric.balanced_accuracy","text":"<pre><code>balanced_accuracy(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the accuracy metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import balanced_accuracy\n&gt;&gt;&gt; balanced_accuracy(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'balanced_accuracy': 1.0, 'count': 5}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.binary_average_precision","title":"arkas.metric.binary_average_precision","text":"<pre><code>binary_average_precision(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the average precision metrics for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, *)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, *)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import binary_average_precision\n&gt;&gt;&gt; metrics = binary_average_precision(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_score=np.array([2, -1, 0, 3, 1])\n... )\n&gt;&gt;&gt; metrics\n{'average_precision': 1.0, 'count': 5}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.binary_confusion_matrix","title":"arkas.metric.binary_confusion_matrix","text":"<pre><code>binary_confusion_matrix(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the confusion matrix metrics for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import binary_confusion_matrix\n&gt;&gt;&gt; binary_confusion_matrix(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n{'confusion_matrix': array([[2, 0], [0, 3]]),\n 'count': 5,\n 'false_negative_rate': 0.0,\n 'false_negative': 0,\n 'false_positive_rate': 0.0,\n 'false_positive': 0,\n 'true_negative_rate': 1.0,\n 'true_negative': 2,\n 'true_positive_rate': 1.0,\n 'true_positive': 3}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.binary_fbeta_score","title":"arkas.metric.binary_fbeta_score","text":"<pre><code>binary_fbeta_score(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    betas: Sequence[float] = (1),\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the F-beta metrics for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>betas</code> <code>Sequence[float]</code> <p>The betas used to compute the F-beta scores.</p> <code>(1)</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import binary_fbeta_score\n&gt;&gt;&gt; binary_fbeta_score(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_pred=np.array([1, 0, 0, 1, 1]),\n... )\n{'count': 5, 'f1': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.binary_jaccard","title":"arkas.metric.binary_jaccard","text":"<pre><code>binary_jaccard(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the Jaccard metrics for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import binary_jaccard\n&gt;&gt;&gt; binary_jaccard(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'count': 5, 'jaccard': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.binary_precision","title":"arkas.metric.binary_precision","text":"<pre><code>binary_precision(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the precision metrics for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import binary_precision\n&gt;&gt;&gt; binary_precision(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'count': 5, 'precision': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.binary_recall","title":"arkas.metric.binary_recall","text":"<pre><code>binary_recall(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the recall metrics for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import binary_recall\n&gt;&gt;&gt; binary_recall(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'count': 5, 'recall': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.binary_roc_auc","title":"arkas.metric.binary_roc_auc","text":"<pre><code>binary_roc_auc(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the Area Under the Receiver Operating Characteristic Curve (ROC AUC) metrics for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p>"},{"location":"refs/metric/#arkas.metric.confusion_matrix","title":"arkas.metric.confusion_matrix","text":"<pre><code>confusion_matrix(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    label_type: str = \"auto\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the confusion matrix metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels.</p> required <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, and <code>'multilabel'</code>. If <code>'binary'</code> or <code>'multilabel'</code>, <code>y_true</code> values  must be <code>0</code> and <code>1</code>.</p> <code>'auto'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import confusion_matrix\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; confusion_matrix(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_pred=np.array([1, 0, 0, 1, 1]),\n...     label_type=\"binary\",\n... )\n{'confusion_matrix': array([[2, 0], [0, 3]]),\n 'count': 5,\n 'false_negative_rate': 0.0,\n 'false_negative': 0,\n 'false_positive_rate': 0.0,\n 'false_positive': 0,\n 'true_negative_rate': 1.0,\n 'true_negative': 2,\n 'true_positive_rate': 1.0,\n 'true_positive': 3}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; confusion_matrix(\n...     y_true=np.array([0, 1, 1, 2, 2, 2]),\n...     y_pred=np.array([0, 1, 1, 2, 2, 2]),\n...     label_type=\"multiclass\",\n... )\n{'confusion_matrix': array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]), 'count': 6}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; confusion_matrix(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     label_type=\"multilabel\",\n... )\n{'confusion_matrix': array([[[2, 0], [0, 3]],\n                            [[3, 0], [0, 2]],\n                            [[2, 0], [0, 3]]]),\n 'count': 5}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.fbeta_score","title":"arkas.metric.fbeta_score","text":"<pre><code>fbeta_score(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    betas: Sequence[float] = (1),\n    label_type: str = \"auto\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the fbeta metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>betas</code> <code>Sequence[float]</code> <p>The betas used to compute the F-beta scores.</p> <code>(1)</code> <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, and <code>'multilabel'</code>. If <code>'binary'</code> or <code>'multilabel'</code>, <code>y_true</code> values  must be <code>0</code> and <code>1</code>.</p> <code>'auto'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import fbeta_score\n&gt;&gt;&gt; # auto\n&gt;&gt;&gt; fbeta_score(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'count': 5, 'f1': 1.0}\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; fbeta_score(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_pred=np.array([1, 0, 0, 1, 1]),\n...     label_type=\"binary\",\n... )\n{'count': 5, 'f1': 1.0}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; fbeta_score(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n...     label_type=\"multiclass\",\n... )\n{'count': 6,\n 'f1': array([1., 1., 1.]),\n 'macro_f1': 1.0,\n 'micro_f1': 1.0,\n 'weighted_f1': 1.0}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; fbeta_score(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     label_type=\"multilabel\",\n... )\n{'count': 5,\n 'f1': array([1., 1., 1.]),\n 'macro_f1': 1.0,\n 'micro_f1': 1.0,\n 'weighted_f1': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.jaccard","title":"arkas.metric.jaccard","text":"<pre><code>jaccard(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    label_type: str = \"auto\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the Jaccard metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, and <code>'multilabel'</code>. If <code>'binary'</code> or <code>'multilabel'</code>, <code>y_true</code> values  must be <code>0</code> and <code>1</code>.</p> <code>'auto'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import jaccard\n&gt;&gt;&gt; # auto\n&gt;&gt;&gt; jaccard(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'count': 5, 'jaccard': 1.0}\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; jaccard(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_pred=np.array([1, 0, 0, 1, 1]),\n...     label_type=\"binary\",\n... )\n{'count': 5, 'jaccard': 1.0}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; jaccard(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n...     label_type=\"multiclass\",\n... )\n{'count': 6,\n 'jaccard': array([1., 1., 1.]),\n 'macro_jaccard': 1.0,\n 'micro_jaccard': 1.0,\n 'weighted_jaccard': 1.0}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; jaccard(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     label_type=\"multilabel\",\n... )\n{'count': 5,\n 'jaccard': array([1., 1., 1.]),\n 'macro_jaccard': 1.0,\n 'micro_jaccard': 1.0,\n 'weighted_jaccard': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.mean_absolute_error","title":"arkas.metric.mean_absolute_error","text":"<pre><code>mean_absolute_error(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the mean absolute error (MAE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import mean_absolute_error\n&gt;&gt;&gt; mean_absolute_error(y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5]))\n{'count': 5, 'mean_absolute_error': 0.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.mean_absolute_percentage_error","title":"arkas.metric.mean_absolute_percentage_error","text":"<pre><code>mean_absolute_percentage_error(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the mean absolute percentage error (MAPE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import mean_absolute_percentage_error\n&gt;&gt;&gt; mean_absolute_percentage_error(\n...     y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5])\n... )\n{'count': 5, 'mean_absolute_percentage_error': 0.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.mean_squared_error","title":"arkas.metric.mean_squared_error","text":"<pre><code>mean_squared_error(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the mean squared error (MSE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import mean_squared_error\n&gt;&gt;&gt; mean_squared_error(y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5]))\n{'count': 5, 'mean_squared_error': 0.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.mean_squared_log_error","title":"arkas.metric.mean_squared_log_error","text":"<pre><code>mean_squared_log_error(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the mean squared logarithmic error (MSLE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import mean_squared_log_error\n&gt;&gt;&gt; mean_squared_log_error(\n...     y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5])\n... )\n{'count': 5, 'mean_squared_log_error': 0.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.mean_tweedie_deviance","title":"arkas.metric.mean_tweedie_deviance","text":"<pre><code>mean_tweedie_deviance(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    powers: Sequence[float] = (0),\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the mean squared error (MSE).</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>powers</code> <code>Sequence[float]</code> <p>The Tweedie power parameter. The higher power the less weight is given to extreme deviations between true and predicted targets.</p> <code>(0)</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import mean_tweedie_deviance\n&gt;&gt;&gt; mean_tweedie_deviance(\n...     y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5])\n... )\n{'count': 5, 'mean_tweedie_deviance_power_0': 0.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.median_absolute_error","title":"arkas.metric.median_absolute_error","text":"<pre><code>median_absolute_error(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the median absolute error.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import median_absolute_error\n&gt;&gt;&gt; median_absolute_error(\n...     y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5])\n... )\n{'count': 5, 'median_absolute_error': 0.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.multiclass_average_precision","title":"arkas.metric.multiclass_average_precision","text":"<pre><code>multiclass_average_precision(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the average precision metrics for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import multiclass_average_precision\n&gt;&gt;&gt; metrics = multiclass_average_precision(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_score=np.array(\n...         [\n...             [0.7, 0.2, 0.1],\n...             [0.4, 0.3, 0.3],\n...             [0.1, 0.8, 0.1],\n...             [0.2, 0.3, 0.5],\n...             [0.4, 0.4, 0.2],\n...             [0.1, 0.2, 0.7],\n...         ]\n...     ),\n... )\n&gt;&gt;&gt; metrics\n{'average_precision': array([0.833..., 0.75 , 0.75 ]),\n 'count': 6,\n 'macro_average_precision': 0.777...,\n 'micro_average_precision': 0.75,\n 'weighted_average_precision': 0.777...}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.multiclass_confusion_matrix","title":"arkas.metric.multiclass_confusion_matrix","text":"<pre><code>multiclass_confusion_matrix(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the confusion matrix metrics for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import multiclass_confusion_matrix\n&gt;&gt;&gt; multiclass_confusion_matrix(\n...     y_true=np.array([0, 1, 1, 2, 2, 2]), y_pred=np.array([0, 1, 1, 2, 2, 2])\n... )\n{'confusion_matrix': array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]), 'count': 6}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.multiclass_fbeta_score","title":"arkas.metric.multiclass_fbeta_score","text":"<pre><code>multiclass_fbeta_score(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    betas: Sequence[float] = (1),\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the F-beta metrics for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>betas</code> <code>Sequence[float]</code> <p>The betas used to compute the F-beta scores.</p> <code>(1)</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import multiclass_fbeta_score\n&gt;&gt;&gt; multiclass_fbeta_score(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n... )\n{'count': 6,\n 'f1': array([1., 1., 1.]),\n 'macro_f1': 1.0,\n 'micro_f1': 1.0,\n 'weighted_f1': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.multiclass_jaccard","title":"arkas.metric.multiclass_jaccard","text":"<pre><code>multiclass_jaccard(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the Jaccard metrics for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import multiclass_jaccard\n&gt;&gt;&gt; multiclass_jaccard(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]), y_pred=np.array([0, 0, 1, 1, 2, 2])\n... )\n{'count': 6,\n 'jaccard': array([1., 1., 1.]),\n 'macro_jaccard': 1.0,\n 'micro_jaccard': 1.0,\n 'weighted_jaccard': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.multiclass_precision","title":"arkas.metric.multiclass_precision","text":"<pre><code>multiclass_precision(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the precision metrics for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import multiclass_precision\n&gt;&gt;&gt; multiclass_precision(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]), y_pred=np.array([0, 0, 1, 1, 2, 2])\n... )\n{'count': 6,\n 'macro_precision': 1.0,\n 'micro_precision': 1.0,\n 'precision': array([1., 1., 1.]),\n 'weighted_precision': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.multiclass_recall","title":"arkas.metric.multiclass_recall","text":"<pre><code>multiclass_recall(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the recall metrics for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import multiclass_recall\n&gt;&gt;&gt; multiclass_recall(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]), y_pred=np.array([0, 0, 1, 1, 2, 2])\n... )\n{'count': 6,\n 'macro_recall': 1.0,\n 'micro_recall': 1.0,\n 'recall': array([1., 1., 1.]),\n 'weighted_recall': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.multiclass_roc_auc","title":"arkas.metric.multiclass_roc_auc","text":"<pre><code>multiclass_roc_auc(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the Area Under the Receiver Operating Characteristic Curve (ROC AUC) metrics for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p>"},{"location":"refs/metric/#arkas.metric.multilabel_average_precision","title":"arkas.metric.multilabel_average_precision","text":"<pre><code>multilabel_average_precision(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the average precision metrics for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import multilabel_average_precision\n&gt;&gt;&gt; metrics = multilabel_average_precision(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_score=np.array([[2, -1, -1], [-1, 1, 2], [0, 2, 3], [3, -2, -4], [1, -3, -5]]),\n... )\n&gt;&gt;&gt; metrics\n{'average_precision': array([1. , 1. , 0.477...]),\n 'count': 5,\n 'macro_average_precision': 0.825...,\n 'micro_average_precision': 0.588...,\n 'weighted_average_precision': 0.804...}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.multilabel_confusion_matrix","title":"arkas.metric.multilabel_confusion_matrix","text":"<pre><code>multilabel_confusion_matrix(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the confusion matrix metrics for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import multilabel_confusion_matrix\n&gt;&gt;&gt; multilabel_confusion_matrix(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n... )\n{'confusion_matrix': array([[[2, 0], [0, 3]],\n                            [[3, 0], [0, 2]],\n                            [[2, 0], [0, 3]]]),\n 'count': 5}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.multilabel_fbeta_score","title":"arkas.metric.multilabel_fbeta_score","text":"<pre><code>multilabel_fbeta_score(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    betas: Sequence[float] = (1),\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the F-beta metrics for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>betas</code> <code>Sequence[float]</code> <p>The betas used to compute the F-beta scores.</p> <code>(1)</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import multilabel_fbeta_score\n&gt;&gt;&gt; multilabel_fbeta_score(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n... )\n{'count': 5,\n 'f1': array([1., 1., 1.]),\n 'macro_f1': 1.0,\n 'micro_f1': 1.0,\n 'weighted_f1': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.multilabel_jaccard","title":"arkas.metric.multilabel_jaccard","text":"<pre><code>multilabel_jaccard(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the Jaccard metrics for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import multilabel_jaccard\n&gt;&gt;&gt; multilabel_jaccard(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n... )\n{'count': 5,\n 'jaccard': array([1., 1., 1.]),\n 'macro_jaccard': 1.0,\n 'micro_jaccard': 1.0,\n 'weighted_jaccard': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.multilabel_precision","title":"arkas.metric.multilabel_precision","text":"<pre><code>multilabel_precision(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the precision metrics for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import multilabel_precision\n&gt;&gt;&gt; multilabel_precision(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n... )\n{'count': 5,\n 'macro_precision': 1.0,\n 'micro_precision': 1.0,\n 'precision': array([1., 1., 1.]),\n 'weighted_precision': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.multilabel_recall","title":"arkas.metric.multilabel_recall","text":"<pre><code>multilabel_recall(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the recall metrics for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import multilabel_recall\n&gt;&gt;&gt; multilabel_recall(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n... )\n{'count': 5,\n 'macro_recall': 1.0,\n 'micro_recall': 1.0,\n 'recall': array([1., 1., 1.]),\n 'weighted_recall': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.multilabel_roc_auc","title":"arkas.metric.multilabel_roc_auc","text":"<pre><code>multilabel_roc_auc(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the Area Under the Receiver Operating Characteristic Curve (ROC AUC) metrics for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p>"},{"location":"refs/metric/#arkas.metric.pearsonr","title":"arkas.metric.pearsonr","text":"<pre><code>pearsonr(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    alternative: str = \"two-sided\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the Pearson correlation coefficient and p-value for testing non-correlation.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>alternative</code> <code>str</code> <p>The alternative hypothesis. Default is 'two-sided'. The following options are available: - 'two-sided': the correlation is nonzero - 'less': the correlation is negative (less than zero) - 'greater': the correlation is positive (greater than zero)</p> <code>'two-sided'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import pearsonr\n&gt;&gt;&gt; pearsonr(y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5]))\n{'count': 5, 'pearson_coeff': 1.0, 'pearson_pvalue': 0.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.precision","title":"arkas.metric.precision","text":"<pre><code>precision(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    label_type: str = \"auto\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the precision metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, and <code>'multilabel'</code>. If <code>'binary'</code> or <code>'multilabel'</code>, <code>y_true</code> values  must be <code>0</code> and <code>1</code>.</p> <code>'auto'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import precision\n&gt;&gt;&gt; # auto\n&gt;&gt;&gt; precision(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'count': 5, 'precision': 1.0}\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; precision(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_pred=np.array([1, 0, 0, 1, 1]),\n...     label_type=\"binary\",\n... )\n{'count': 5, 'precision': 1.0}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; precision(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n...     label_type=\"multiclass\",\n... )\n{'count': 6,\n 'macro_precision': 1.0,\n 'micro_precision': 1.0,\n 'precision': array([1., 1., 1.]),\n 'weighted_precision': 1.0}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; precision(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     label_type=\"multilabel\",\n... )\n{'count': 5,\n 'macro_precision': 1.0,\n 'micro_precision': 1.0,\n 'precision': array([1., 1., 1.]),\n 'weighted_precision': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.recall","title":"arkas.metric.recall","text":"<pre><code>recall(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    label_type: str = \"auto\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the recall metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, and <code>'multilabel'</code>. If <code>'binary'</code> or <code>'multilabel'</code>, <code>y_true</code> values  must be <code>0</code> and <code>1</code>.</p> <code>'auto'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import recall\n&gt;&gt;&gt; # auto\n&gt;&gt;&gt; recall(y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1]))\n{'count': 5, 'recall': 1.0}\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; recall(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_pred=np.array([1, 0, 0, 1, 1]),\n...     label_type=\"binary\",\n... )\n{'count': 5, 'recall': 1.0}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; recall(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n...     label_type=\"multiclass\",\n... )\n{'count': 6,\n 'macro_recall': 1.0,\n 'micro_recall': 1.0,\n 'recall': array([1., 1., 1.]),\n 'weighted_recall': 1.0}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; recall(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     label_type=\"multilabel\",\n... )\n{'count': 5,\n 'macro_recall': 1.0,\n 'micro_recall': 1.0,\n 'recall': array([1., 1., 1.]),\n 'weighted_recall': 1.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.regression_errors","title":"arkas.metric.regression_errors","text":"<pre><code>regression_errors(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the regression error metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import regression_errors\n&gt;&gt;&gt; regression_errors(y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5]))\n{'count': 5,\n 'mean_absolute_error': 0.0,\n 'median_absolute_error': 0.0,\n 'mean_squared_error': 0.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.roc_auc","title":"arkas.metric.roc_auc","text":"<pre><code>roc_auc(\n    y_true: ndarray,\n    y_score: ndarray,\n    *,\n    label_type: str = \"auto\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float | ndarray]\n</code></pre> <p>Return the Area Under the Receiver Operating Characteristic Curve (ROC AUC) metrics.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, and <code>'multilabel'</code>. If <code>'binary'</code> or <code>'multilabel'</code>, <code>y_true</code> values  must be <code>0</code> and <code>1</code>.</p> <code>'auto'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float | ndarray]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import roc_auc\n&gt;&gt;&gt; # auto\n&gt;&gt;&gt; metrics = roc_auc(y_true=np.array([1, 0, 0, 1, 1]), y_score=np.array([2, -1, 0, 3, 1]))\n&gt;&gt;&gt; metrics\n{'count': 5, 'roc_auc': 1.0}\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; metrics = roc_auc(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_score=np.array([2, -1, 0, 3, 1]),\n...     label_type=\"binary\",\n... )\n&gt;&gt;&gt; metrics\n{'count': 5, 'roc_auc': 1.0}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; metrics = roc_auc(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_score=np.array(\n...         [\n...             [0.7, 0.2, 0.1],\n...             [0.4, 0.3, 0.3],\n...             [0.1, 0.8, 0.1],\n...             [0.2, 0.3, 0.5],\n...             [0.4, 0.4, 0.2],\n...             [0.1, 0.2, 0.7],\n...         ]\n...     ),\n...     label_type=\"multiclass\",\n... )\n&gt;&gt;&gt; metrics\n{'count': 6,\n 'macro_roc_auc': 0.833...,\n 'micro_roc_auc': 0.826...,\n 'roc_auc': array([0.9375, 0.8125, 0.75  ]),\n 'weighted_roc_auc': 0.833...}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; metrics = roc_auc(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_score=np.array([[2, -1, -1], [-1, 1, 2], [0, 2, 3], [3, -2, -4], [1, -3, -5]]),\n...     label_type=\"multilabel\",\n... )\n&gt;&gt;&gt; metrics\n{'count': 5,\n 'macro_roc_auc': 0.666...,\n 'micro_roc_auc': 0.544...,\n 'roc_auc': array([1., 1., 0.]),\n 'weighted_roc_auc': 0.625}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.spearmanr","title":"arkas.metric.spearmanr","text":"<pre><code>spearmanr(\n    y_true: ndarray,\n    y_pred: ndarray,\n    *,\n    alternative: str = \"two-sided\",\n    prefix: str = \"\",\n    suffix: str = \"\",\n    ignore_nan: bool = False\n) -&gt; dict[str, float]\n</code></pre> <p>Return the Spearman correlation coefficient and p-value for testing non-correlation.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>alternative</code> <code>str</code> <p>The alternative hypothesis. Default is 'two-sided'. The following options are available: - 'two-sided': the correlation is nonzero - 'less': the correlation is negative (less than zero) - 'greater': the correlation is positive (greater than zero)</p> <code>'two-sided'</code> <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <code>ignore_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are ignored while computing the metrics, otherwise an exception is raised.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The computed metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric import spearmanr\n&gt;&gt;&gt; spearmanr(\n...     y_true=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n...     y_pred=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n... )\n{'count': 9, 'spearman_coeff': 1.0, 'spearman_pvalue': 0.0}\n</code></pre>"},{"location":"refs/metric/#arkas.metric.utils","title":"arkas.metric.utils","text":"<p>Contain utility functions to compute metrics.</p>"},{"location":"refs/metric/#arkas.metric.utils.check_label_type","title":"arkas.metric.utils.check_label_type","text":"<pre><code>check_label_type(label_type: str) -&gt; None\n</code></pre> <p>Check if the label type value is valid or not.</p> <p>Parameters:</p> Name Type Description Default <code>label_type</code> <code>str</code> <p>The type of labels. The valid values are <code>'binary'</code>, <code>'multiclass'</code>, <code>'multilabel'</code>, and <code>'auto'</code>.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if an invalid value is passed to <code>label_type</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.metric.utils import check_label_type\n&gt;&gt;&gt; check_label_type(label_type=\"binary\")\n</code></pre>"},{"location":"refs/metric/#arkas.metric.utils.check_same_shape_pred","title":"arkas.metric.utils.check_same_shape_pred","text":"<pre><code>check_same_shape_pred(\n    y_true: ndarray, y_pred: ndarray\n) -&gt; None\n</code></pre> <p>Check if <code>y_true</code> and <code>y_pred</code> arrays have the same shape.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p><code>'y_true'</code> and <code>'y_pred'</code> have different shapes.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric.utils import check_same_shape_pred\n&gt;&gt;&gt; y_true = np.array([1, 0, 0, 1])\n&gt;&gt;&gt; y_pred = np.array([0, 1, 0, 1])\n&gt;&gt;&gt; check_same_shape_pred(y_true, y_pred)\n</code></pre>"},{"location":"refs/metric/#arkas.metric.utils.check_same_shape_score","title":"arkas.metric.utils.check_same_shape_score","text":"<pre><code>check_same_shape_score(\n    y_true: ndarray, y_score: ndarray\n) -&gt; None\n</code></pre> <p>Check if <code>y_true</code> and <code>y_score</code> arrays have the same shape.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p><code>'y_true'</code> and <code>'y_score'</code> have different shapes.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric.utils import check_same_shape_score\n&gt;&gt;&gt; y_true = np.array([1, 0, 0, 1])\n&gt;&gt;&gt; y_score = np.array([0, 1, 0, 1])\n&gt;&gt;&gt; check_same_shape_score(y_true, y_score)\n</code></pre>"},{"location":"refs/metric/#arkas.metric.utils.multi_isnan","title":"arkas.metric.utils.multi_isnan","text":"<pre><code>multi_isnan(arrays: Sequence[ndarray]) -&gt; ndarray\n</code></pre> <p>Test element-wise for NaN for all input arrays and return result as a boolean array.</p> <p>Parameters:</p> Name Type Description Default <code>arrays</code> <code>Sequence[ndarray]</code> <p>The input arrays to test. All the arrays must have the same shape.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A boolean array. <code>True</code> where any array is NaN, <code>False</code> otherwise.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric.utils import multi_isnan\n&gt;&gt;&gt; mask = multi_isnan(\n...     [np.array([1, 0, 0, 1, float(\"nan\")]), np.array([1, float(\"nan\"), 0, 1, 1])]\n... )\n&gt;&gt;&gt; mask\narray([False,  True, False, False,  True])\n</code></pre>"},{"location":"refs/metric/#arkas.metric.utils.preprocess_pred","title":"arkas.metric.utils.preprocess_pred","text":"<pre><code>preprocess_pred(\n    y_true: ndarray,\n    y_pred: ndarray,\n    remove_nan: bool = False,\n) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Preprocess <code>y_true</code> and <code>y_pred</code> arrays.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels.</p> required <code>remove_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are removed, otherwise they are kept.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>A tuple with the preprocessed <code>y_true</code> and <code>y_pred</code> arrays.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if an invalid value is passed to <code>nan</code>.</p> <code>RuntimeError</code> <p><code>'y_true'</code> and <code>'y_pred'</code> have different shapes.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric.utils import preprocess_pred\n&gt;&gt;&gt; y_true = np.array([1, 0, 0, 1, 1, float(\"nan\")])\n&gt;&gt;&gt; y_pred = np.array([0, 1, 0, 1, float(\"nan\"), 1])\n&gt;&gt;&gt; preprocess_pred(y_true, y_pred)\n(array([ 1.,  0.,  0.,  1.,  1., nan]), array([ 0.,  1.,  0.,  1., nan,  1.]))\n&gt;&gt;&gt; preprocess_pred(y_true, y_pred, remove_nan=True)\n(array([1., 0., 0., 1.]), array([0., 1., 0., 1.]))\n</code></pre>"},{"location":"refs/metric/#arkas.metric.utils.preprocess_pred_multilabel","title":"arkas.metric.utils.preprocess_pred_multilabel","text":"<pre><code>preprocess_pred_multilabel(\n    y_true: ndarray,\n    y_pred: ndarray,\n    remove_nan: bool = False,\n) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Preprocess <code>y_true</code> and <code>y_pred</code> arrays.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels.</p> required <code>remove_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are removed, otherwise they are kept.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>A tuple with the preprocessed <code>y_true</code> and <code>y_pred</code> arrays.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if an invalid value is passed to <code>nan</code>.</p> <code>RuntimeError</code> <p><code>'y_true'</code> and <code>'y_pred'</code> have different shapes.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric.utils import preprocess_pred_multilabel\n&gt;&gt;&gt; y_true = np.array([[1, float(\"nan\"), 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]])\n&gt;&gt;&gt; y_pred = np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, float(\"nan\")]])\n&gt;&gt;&gt; preprocess_pred_multilabel(y_true, y_pred)\n(array([[ 1., nan,  1.],\n        [ 0.,  1.,  0.],\n        [ 0.,  1.,  0.],\n        [ 1.,  0.,  1.],\n        [ 1.,  0.,  1.]]),\n array([[ 1.,  0.,  1.],\n        [ 0.,  1.,  0.],\n        [ 0.,  1.,  0.],\n        [ 1.,  0.,  1.],\n        [ 1.,  0., nan]]))\n&gt;&gt;&gt; preprocess_pred_multilabel(y_true, y_pred, remove_nan=True)\n(array([[0., 1., 0.],\n        [0., 1., 0.],\n        [1., 0., 1.]]),\n array([[0., 1., 0.],\n        [0., 1., 0.],\n        [1., 0., 1.]]))\n</code></pre>"},{"location":"refs/metric/#arkas.metric.utils.preprocess_score_binary","title":"arkas.metric.utils.preprocess_score_binary","text":"<pre><code>preprocess_score_binary(\n    y_true: ndarray,\n    y_score: ndarray,\n    remove_nan: bool = False,\n) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Preprocess <code>y_true</code> and <code>y_score</code> arrays for the binary classification case.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, *)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, *)</code>.</p> required <code>remove_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are removed, otherwise they are kept.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>A tuple with the preprocessed <code>y_true</code> and <code>y_score</code> arrays.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric.utils import preprocess_score_binary\n&gt;&gt;&gt; y_true = np.array([1, 0, 0, 1, 1, float(\"nan\")])\n&gt;&gt;&gt; y_score = np.array([0, 1, 0, 1, float(\"nan\"), 1])\n&gt;&gt;&gt; preprocess_score_binary(y_true, y_score)\n(array([ 1.,  0.,  0.,  1.,  1., nan]), array([ 0.,  1.,  0.,  1., nan,  1.]))\n&gt;&gt;&gt; preprocess_score_binary(y_true, y_score, remove_nan=True)\n(array([1., 0., 0., 1.]), array([0., 1., 0., 1.]))\n</code></pre>"},{"location":"refs/metric/#arkas.metric.utils.preprocess_score_multiclass","title":"arkas.metric.utils.preprocess_score_multiclass","text":"<pre><code>preprocess_score_multiclass(\n    y_true: ndarray,\n    y_score: ndarray,\n    remove_nan: bool = False,\n) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Preprocess <code>y_true</code> and <code>y_score</code> arrays for the multiclass classification case.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, 1)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <code>remove_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are removed, otherwise they are kept.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>A tuple with the preprocessed <code>y_true</code> and <code>y_score</code> arrays.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric.utils import preprocess_score_multiclass\n&gt;&gt;&gt; y_true = np.array([0, 0, 1, 1, 2, float(\"nan\")])\n&gt;&gt;&gt; y_score = np.array(\n...     [\n...         [0.7, 0.2, 0.1],\n...         [0.4, 0.3, 0.3],\n...         [0.1, 0.8, float(\"nan\")],\n...         [0.2, 0.3, 0.5],\n...         [0.4, 0.4, 0.2],\n...         [0.1, 0.2, 0.7],\n...     ]\n... )\n&gt;&gt;&gt; preprocess_score_multiclass(y_true, y_score)\n(array([ 0.,  0.,  1.,  1.,  2., nan]),\n array([[0.7, 0.2, 0.1],\n        [0.4, 0.3, 0.3],\n        [0.1, 0.8, nan],\n        [0.2, 0.3, 0.5],\n        [0.4, 0.4, 0.2],\n        [0.1, 0.2, 0.7]]))\n&gt;&gt;&gt; preprocess_score_multiclass(y_true, y_score, remove_nan=True)\n(array([0., 0., 1., 2.]),\n array([[0.7, 0.2, 0.1],\n        [0.4, 0.3, 0.3],\n        [0.2, 0.3, 0.5],\n        [0.4, 0.4, 0.2]]))\n</code></pre>"},{"location":"refs/metric/#arkas.metric.utils.preprocess_score_multilabel","title":"arkas.metric.utils.preprocess_score_multilabel","text":"<pre><code>preprocess_score_multilabel(\n    y_true: ndarray,\n    y_score: ndarray,\n    remove_nan: bool = False,\n) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Preprocess <code>y_true</code> and <code>y_score</code> arrays for the multilabel classification case.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code> or <code>(n_samples,)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, n_classes)</code> or <code>(n_samples,)</code>.</p> required <code>remove_nan</code> <code>bool</code> <p>If <code>True</code>, the NaN values are removed, otherwise they are kept.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>A tuple with the preprocessed <code>y_true</code> and <code>y_score</code> arrays.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.metric.utils import preprocess_score_multilabel\n&gt;&gt;&gt; y_true = np.array([[1, float(\"nan\"), 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]])\n&gt;&gt;&gt; y_score = np.array(\n...     [[2, -1, -1], [-1, 1, 2], [0, 2, 3], [3, -2, -4], [1, float(\"nan\"), -5]]\n... )\n&gt;&gt;&gt; preprocess_score_multilabel(y_true, y_score)\n(array([[ 1., nan,  1.],\n        [ 0.,  1.,  0.],\n        [ 0.,  1.,  0.],\n        [ 1.,  0.,  1.],\n        [ 1.,  0.,  1.]]),\n array([[ 2., -1., -1.],\n        [-1.,  1.,  2.],\n        [ 0.,  2.,  3.],\n        [ 3., -2., -4.],\n        [ 1., nan, -5.]]))\n&gt;&gt;&gt; preprocess_score_multilabel(y_true, y_score, remove_nan=True)\n(array([[0., 1., 0.],\n        [0., 1., 0.],\n        [1., 0., 1.]]),\n array([[-1.,  1.,  2.],\n        [ 0.,  2.,  3.],\n        [ 3., -2., -4.]]))\n</code></pre>"},{"location":"refs/plot/","title":"arkas.plot","text":""},{"location":"refs/plot/#arkas.plot","title":"arkas.plot","text":"<p>Contain plotting functionalities.</p>"},{"location":"refs/plot/#arkas.plot.binary_precision_recall_curve","title":"arkas.plot.binary_precision_recall_curve","text":"<pre><code>binary_precision_recall_curve(\n    ax: Axes,\n    y_true: ndarray,\n    y_pred: ndarray,\n    **kwargs: Any\n) -&gt; None\n</code></pre> <p>Plot the precision-recall curve for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axes of the matplotlib figure to update.</p> required <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code> with <code>0</code> and <code>1</code> values.</p> required <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments that are passed to <code>PrecisionRecallDisplay.from_predictions</code>.</p> <code>{}</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from matplotlib import pyplot as plt\n&gt;&gt;&gt; from arkas.plot import binary_precision_recall_curve\n&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; binary_precision_recall_curve(\n...     ax=ax, y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n</code></pre>"},{"location":"refs/plot/#arkas.plot.binary_roc_curve","title":"arkas.plot.binary_roc_curve","text":"<pre><code>binary_roc_curve(\n    ax: Axes,\n    y_true: ndarray,\n    y_score: ndarray,\n    **kwargs: Any\n) -&gt; None\n</code></pre> <p>Plot the Receiver Operating Characteristic Curve (ROC) for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The axes of the matplotlib figure to update.</p> required <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples,)</code>.</p> required <code>**kwargs</code> <code>Any</code> <p>Arbitrary keyword arguments that are passed to <code>RocCurveDisplay.from_predictions</code>.</p> <code>{}</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from matplotlib import pyplot as plt\n&gt;&gt;&gt; from arkas.plot import binary_roc_curve\n&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; binary_roc_curve(\n...     ax=ax, y_true=np.array([1, 0, 0, 1, 1]), y_score=np.array([2, -1, 0, 3, 1])\n... )\n</code></pre>"},{"location":"refs/reporter/","title":"arkas.reporter","text":""},{"location":"refs/reporter/#arkas.reporter","title":"arkas.reporter","text":"<p>Contain reporters.</p>"},{"location":"refs/result/","title":"arkas.result","text":""},{"location":"refs/result/#arkas.result","title":"arkas.result","text":"<p>Contain results.</p>"},{"location":"refs/result/#arkas.result.AccuracyResult","title":"arkas.result.AccuracyResult","text":"<p>               Bases: <code>BaseResult</code></p> <p>Implement the accuracy result.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> where the values are in <code>{0, ..., n_classes-1}</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code> where the values are in <code>{0, ..., n_classes-1}</code>.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import AccuracyResult\n&gt;&gt;&gt; result = AccuracyResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n&gt;&gt;&gt; result\nAccuracyResult(y_true=(5,), y_pred=(5,))\n&gt;&gt;&gt; result.compute_metrics()\n{'accuracy': 1.0, 'count_correct': 5, 'count_incorrect': 0, 'count': 5, 'error': 0.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.AveragePrecisionResult","title":"arkas.result.AveragePrecisionResult","text":"<p>               Bases: <code>BaseResult</code></p> <p>Implement the average precision result.</p> <p>This result can be used in 3 different settings:</p> <ul> <li>binary: <code>y_true</code> must be an array of shape <code>(*)</code>     with <code>0</code> and <code>1</code> values, and <code>y_score</code> must be an array     of shape <code>(*)</code>.</li> <li>multiclass: <code>y_true</code> must be an array of shape <code>(n_samples,)</code>     with values in <code>{0, ..., n_classes-1}</code>, and <code>y_score</code> must     be an array of shape <code>(n_samples, n_classes)</code>.</li> <li>multilabel: <code>y_true</code> must be an array of shape     <code>(n_samples, n_classes)</code> with <code>0</code> and <code>1</code> values, and     <code>y_score</code> must be an array of shape     <code>(n_samples, n_classes)</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>label_type</code> <code>str</code> <p>The type of labels used to evaluate the metrics. The valid values are: <code>'binary'</code>, <code>'multiclass'</code>, <code>'multilabel'</code>, and <code>'auto'</code>. If <code>'binary'</code> or <code>'multilabel'</code>, <code>y_true</code> values  must be <code>0</code> and <code>1</code>. If <code>'auto'</code>, it tries to automatically find the label type from the arrays' shape.</p> <code>'auto'</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import AveragePrecisionResult\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; result = AveragePrecisionResult(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_score=np.array([2, -1, 0, 3, 1]),\n...     label_type=\"binary\",\n... )\n&gt;&gt;&gt; result\nAveragePrecisionResult(y_true=(5,), y_score=(5,), label_type=binary)\n&gt;&gt;&gt; result.compute_metrics()\n{'average_precision': 1.0, 'count': 5}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; result = AveragePrecisionResult(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_score=np.array([[2, -1, -1], [-1, 1, 2], [0, 2, 3], [3, -2, -4], [1, -3, -5]]),\n...     label_type=\"multilabel\",\n... )\n&gt;&gt;&gt; result\nAveragePrecisionResult(y_true=(5, 3), y_score=(5, 3), label_type=multilabel)\n&gt;&gt;&gt; result.compute_metrics()\n{'average_precision': array([1. , 1. , 0.477...]),\n 'count': 5,\n 'macro_average_precision': 0.825...,\n 'micro_average_precision': 0.588...,\n 'weighted_average_precision': 0.804...}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; result = AveragePrecisionResult(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_score=np.array(\n...         [\n...             [0.7, 0.2, 0.1],\n...             [0.4, 0.3, 0.3],\n...             [0.1, 0.8, 0.1],\n...             [0.2, 0.3, 0.5],\n...             [0.4, 0.4, 0.2],\n...             [0.1, 0.2, 0.7],\n...         ]\n...     ),\n...     label_type=\"multiclass\",\n... )\n&gt;&gt;&gt; result\nAveragePrecisionResult(y_true=(6,), y_score=(6, 3), label_type=multiclass)\n&gt;&gt;&gt; result.compute_metrics()\n{'average_precision': array([0.833..., 0.75 , 0.75 ]),\n 'count': 6,\n 'macro_average_precision': 0.777...,\n 'micro_average_precision': 0.75,\n 'weighted_average_precision': 0.777...}\n&gt;&gt;&gt; # auto\n&gt;&gt;&gt; result = AveragePrecisionResult(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_score=np.array([2, -1, 0, 3, 1]),\n... )\n&gt;&gt;&gt; result\nAveragePrecisionResult(y_true=(5,), y_score=(5,), label_type=binary)\n&gt;&gt;&gt; result.compute_metrics()\n{'average_precision': 1.0, 'count': 5}\n</code></pre>"},{"location":"refs/result/#arkas.result.BalancedAccuracyResult","title":"arkas.result.BalancedAccuracyResult","text":"<p>               Bases: <code>BaseResult</code></p> <p>Implement the balanced accuracy result.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> where the values are in <code>{0, ..., n_classes-1}</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code> where the values are in <code>{0, ..., n_classes-1}</code>.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import AccuracyResult\n&gt;&gt;&gt; result = BalancedAccuracyResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n&gt;&gt;&gt; result\nBalancedAccuracyResult(y_true=(5,), y_pred=(5,))\n&gt;&gt;&gt; result.compute_metrics()\n{'balanced_accuracy': 1.0, 'count': 5}\n</code></pre>"},{"location":"refs/result/#arkas.result.BaseResult","title":"arkas.result.BaseResult","text":"<p>               Bases: <code>ABC</code></p> <p>Define the base class to manage results.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import AccuracyResult\n&gt;&gt;&gt; result = AccuracyResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n&gt;&gt;&gt; result\nAccuracyResult(y_true=(5,), y_pred=(5,))\n&gt;&gt;&gt; result.compute_metrics()\n{'accuracy': 1.0, 'count_correct': 5, 'count_incorrect': 0, 'count': 5, 'error': 0.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.BaseResult.compute_metrics","title":"arkas.result.BaseResult.compute_metrics  <code>abstractmethod</code>","text":"<pre><code>compute_metrics(prefix: str = '', suffix: str = '') -&gt; dict\n</code></pre> <p>Return the metrics associated to the result.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <p>Returns:</p> Type Description <code>dict</code> <p>The metrics.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import AccuracyResult\n&gt;&gt;&gt; result = AccuracyResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n&gt;&gt;&gt; result.compute_metrics()\n{'accuracy': 1.0, 'count_correct': 5, 'count_incorrect': 0, 'count': 5, 'error': 0.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.BaseResult.equal","title":"arkas.result.BaseResult.equal  <code>abstractmethod</code>","text":"<pre><code>equal(other: Any, equal_nan: bool = False) -&gt; bool\n</code></pre> <p>Indicate if two results are equal or not.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Any</code> <p>The other result to compare.</p> required <code>equal_nan</code> <code>bool</code> <p>Whether to compare NaN's as equal. If <code>True</code>, NaN's in both objects will be considered equal.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the two results are equal, otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import AccuracyResult\n&gt;&gt;&gt; res1 = AccuracyResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n&gt;&gt;&gt; res2 = AccuracyResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n&gt;&gt;&gt; res3 = AccuracyResult(\n...     y_true=np.array([1, 0, 0, 0, 0]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n&gt;&gt;&gt; res1.equal(res2)\nTrue\n&gt;&gt;&gt; res1.equal(res3)\nFalse\n</code></pre>"},{"location":"refs/result/#arkas.result.BaseResult.generate_figures","title":"arkas.result.BaseResult.generate_figures  <code>abstractmethod</code>","text":"<pre><code>generate_figures(\n    prefix: str = \"\", suffix: str = \"\"\n) -&gt; dict[str, Figure]\n</code></pre> <p>Return the figures associated to the result.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The key prefix in the returned dictionary.</p> <code>''</code> <code>suffix</code> <code>str</code> <p>The key suffix in the returned dictionary.</p> <code>''</code> <p>Returns:</p> Type Description <code>dict[str, Figure]</code> <p>The figures.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import AccuracyResult\n&gt;&gt;&gt; result = AccuracyResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n&gt;&gt;&gt; result.generate_figures()\n{}\n</code></pre>"},{"location":"refs/result/#arkas.result.BinaryAveragePrecisionResult","title":"arkas.result.BinaryAveragePrecisionResult","text":"<p>               Bases: <code>BaseAveragePrecisionResult</code></p> <p>Implement the average precision result for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, *)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, *)</code>.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import BinaryAveragePrecisionResult\n&gt;&gt;&gt; result = BinaryAveragePrecisionResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_score=np.array([2, -1, 0, 3, 1])\n... )\n&gt;&gt;&gt; result\nBinaryAveragePrecisionResult(y_true=(5,), y_score=(5,))\n&gt;&gt;&gt; result.compute_metrics()\n{'average_precision': 1.0, 'count': 5}\n</code></pre>"},{"location":"refs/result/#arkas.result.BinaryClassificationResult","title":"arkas.result.BinaryClassificationResult","text":"<p>               Bases: <code>BaseResult</code></p> <p>Implement the default binary classification result.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target binary labels. This input must be an array of shape <code>(n_samples,)</code> where the values are <code>0</code> or <code>1</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted binary labels. This input must be an array of shape <code>(n_samples,)</code> where the values are <code>0</code> or <code>1</code>.</p> required <code>y_score</code> <code>ndarray | None</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions.</p> <code>None</code> <code>betas</code> <code>Sequence[float]</code> <p>The betas used to compute the F-beta scores.</p> <code>(1)</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import BinaryClassificationResult\n&gt;&gt;&gt; result = BinaryClassificationResult(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_pred=np.array([1, 0, 0, 1, 1]),\n...     y_score=np.array([2, -1, 0, 3, 1]),\n... )\n&gt;&gt;&gt; result\nBinaryClassificationResult(y_true=(5,), y_pred=(5,), y_score=(5,), betas=(1,))\n&gt;&gt;&gt; result.compute_metrics()\n{'accuracy': 1.0,\n 'count_correct': 5,\n 'count_incorrect': 0,\n 'count': 5,\n 'error': 0.0,\n 'balanced_accuracy': 1.0,\n 'confusion_matrix': array([[2, 0], [0, 3]]),\n 'false_negative_rate': 0.0,\n 'false_negative': 0,\n 'false_positive_rate': 0.0,\n 'false_positive': 0,\n 'true_negative_rate': 1.0,\n 'true_negative': 2,\n 'true_positive_rate': 1.0,\n 'true_positive': 3,\n 'f1': 1.0,\n 'jaccard': 1.0,\n 'precision': 1.0,\n 'recall': 1.0,\n 'average_precision': 1.0,\n 'roc_auc': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.BinaryConfusionMatrixResult","title":"arkas.result.BinaryConfusionMatrixResult","text":"<p>               Bases: <code>BaseConfusionMatrixResult</code></p> <p>Implement the confusion matrix result for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, *)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, *)</code> with <code>0</code> and <code>1</code> values.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import BinaryConfusionMatrixResult\n&gt;&gt;&gt; result = BinaryConfusionMatrixResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n&gt;&gt;&gt; result\nBinaryConfusionMatrixResult(y_true=(5,), y_pred=(5,))\n&gt;&gt;&gt; result.compute_metrics()\n{'confusion_matrix': array([[2, 0], [0, 3]]),\n 'count': 5,\n 'false_negative_rate': 0.0,\n 'false_negative': 0,\n 'false_positive_rate': 0.0,\n 'false_positive': 0,\n 'true_negative_rate': 1.0,\n 'true_negative': 2,\n 'true_positive_rate': 1.0,\n 'true_positive': 3}\n</code></pre>"},{"location":"refs/result/#arkas.result.BinaryFbetaScoreResult","title":"arkas.result.BinaryFbetaScoreResult","text":"<p>               Bases: <code>BaseFbetaScoreResult</code></p> <p>Implement the F-beta result for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, *)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, *)</code> with <code>0</code> and <code>1</code> values.</p> required <code>betas</code> <code>Sequence[float]</code> <p>The betas used to compute the F-beta scores.</p> <code>(1)</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import BinaryFbetaScoreResult\n&gt;&gt;&gt; result = BinaryFbetaScoreResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n&gt;&gt;&gt; result\nBinaryFbetaScoreResult(y_true=(5,), y_pred=(5,), betas=(1,))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5, 'f1': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.BinaryJaccardResult","title":"arkas.result.BinaryJaccardResult","text":"<p>               Bases: <code>BaseJaccardResult</code></p> <p>Implement the Jaccard result for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, *)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, *)</code> with <code>0</code> and <code>1</code> values.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import BinaryJaccardResult\n&gt;&gt;&gt; result = BinaryJaccardResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n&gt;&gt;&gt; result\nBinaryJaccardResult(y_true=(5,), y_pred=(5,))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5, 'jaccard': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.BinaryPrecisionResult","title":"arkas.result.BinaryPrecisionResult","text":"<p>               Bases: <code>BasePrecisionResult</code></p> <p>Implement the precision result for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, *)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, *)</code> with <code>0</code> and <code>1</code> values.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import BinaryPrecisionResult\n&gt;&gt;&gt; result = BinaryPrecisionResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n&gt;&gt;&gt; result\nBinaryPrecisionResult(y_true=(5,), y_pred=(5,))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5, 'precision': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.BinaryRecallResult","title":"arkas.result.BinaryRecallResult","text":"<p>               Bases: <code>BaseRecallResult</code></p> <p>Implement the recall result for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, *)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, *)</code> with <code>0</code> and <code>1</code> values.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import BinaryRecallResult\n&gt;&gt;&gt; result = BinaryRecallResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n&gt;&gt;&gt; result\nBinaryRecallResult(y_true=(5,), y_pred=(5,))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5, 'recall': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.BinaryRocAucResult","title":"arkas.result.BinaryRocAucResult","text":"<p>               Bases: <code>BaseRocAucResult</code></p> <p>Implement the Area Under the Receiver Operating Characteristic Curve (ROC AUC) result for binary labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, *)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, *)</code>.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import BinaryRocAucResult\n&gt;&gt;&gt; result = BinaryRocAucResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_score=np.array([2, -1, 0, 3, 1])\n... )\n&gt;&gt;&gt; result\nBinaryRocAucResult(y_true=(5,), y_score=(5,))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5, 'roc_auc': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.EmptyResult","title":"arkas.result.EmptyResult","text":"<p>               Bases: <code>Result</code></p> <p>Implement an empty result.</p> <p>This result is designed to be used when it is possible to evaluate a result.</p>"},{"location":"refs/result/#arkas.result.MappingResult","title":"arkas.result.MappingResult","text":"<p>               Bases: <code>BaseResult</code></p> <p>Implement a result that combines a mapping of result objects into a single result object.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Mapping[str, BaseResult]</code> <p>The mapping of result objects to combine.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MappingResult, Result\n&gt;&gt;&gt; result = MappingResult(\n...     {\n...         \"class1\": Result(metrics={\"accuracy\": 62.0, \"count\": 42}),\n...         \"class2\": Result(metrics={\"accuracy\": 42.0, \"count\": 42}),\n...     }\n... )\n&gt;&gt;&gt; result\nMappingResult(count=2)\n&gt;&gt;&gt; result.compute_metrics()\n{'class1': {'accuracy': 62.0, 'count': 42}, 'class2': {'accuracy': 42.0, 'count': 42}}\n</code></pre>"},{"location":"refs/result/#arkas.result.MeanAbsoluteErrorResult","title":"arkas.result.MeanAbsoluteErrorResult","text":"<p>               Bases: <code>BaseResult</code></p> <p>Implement the mean absolute error (MAE) result.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MeanAbsoluteErrorResult\n&gt;&gt;&gt; result = MeanAbsoluteErrorResult(\n...     y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5])\n... )\n&gt;&gt;&gt; result\nMeanAbsoluteErrorResult(y_true=(5,), y_pred=(5,))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5, 'mean_absolute_error': 0.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.MeanSquaredErrorResult","title":"arkas.result.MeanSquaredErrorResult","text":"<p>               Bases: <code>BaseResult</code></p> <p>Implement the mean squared error (MSE) result.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MeanSquaredErrorResult\n&gt;&gt;&gt; result = MeanSquaredErrorResult(\n...     y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5])\n... )\n&gt;&gt;&gt; result\nMeanSquaredErrorResult(y_true=(5,), y_pred=(5,))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5, 'mean_squared_error': 0.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.MulticlassAveragePrecisionResult","title":"arkas.result.MulticlassAveragePrecisionResult","text":"<p>               Bases: <code>BaseAveragePrecisionResult</code></p> <p>Implement the average precision result for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MulticlassAveragePrecisionResult\n&gt;&gt;&gt; result = MulticlassAveragePrecisionResult(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_score=np.array(\n...         [\n...             [0.7, 0.2, 0.1],\n...             [0.4, 0.3, 0.3],\n...             [0.1, 0.8, 0.1],\n...             [0.2, 0.5, 0.3],\n...             [0.3, 0.3, 0.4],\n...             [0.1, 0.2, 0.7],\n...         ]\n...     ),\n... )\n&gt;&gt;&gt; result\nMulticlassAveragePrecisionResult(y_true=(6,), y_score=(6, 3))\n&gt;&gt;&gt; result.compute_metrics()\n{'average_precision': array([1., 1., 1.]),\n 'count': 6,\n 'macro_average_precision': 1.0,\n 'micro_average_precision': 1.0,\n 'weighted_average_precision': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.MulticlassConfusionMatrixResult","title":"arkas.result.MulticlassConfusionMatrixResult","text":"<p>               Bases: <code>BaseConfusionMatrixResult</code></p> <p>Implement the confusion matrix result for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, *)</code> with values in <code>{0, ..., n_classes-1}</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, *)</code> with values in <code>{0, ..., n_classes-1}</code>.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MulticlassConfusionMatrixResult\n&gt;&gt;&gt; result = MulticlassConfusionMatrixResult(\n...     y_true=np.array([0, 1, 1, 2, 2, 2]),\n...     y_pred=np.array([0, 1, 1, 2, 2, 2]),\n... )\n&gt;&gt;&gt; result\nMulticlassConfusionMatrixResult(y_true=(6,), y_pred=(6,))\n&gt;&gt;&gt; result.compute_metrics()\n{'confusion_matrix': array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]), 'count': 6}\n</code></pre>"},{"location":"refs/result/#arkas.result.MulticlassFbetaScoreResult","title":"arkas.result.MulticlassFbetaScoreResult","text":"<p>               Bases: <code>BaseFbetaScoreResult</code></p> <p>Implement the F-beta result for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, *)</code> with values in <code>{0, ..., n_classes-1}</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, *)</code> with values in <code>{0, ..., n_classes-1}</code>.</p> required <code>betas</code> <code>Sequence[float]</code> <p>The betas used to compute the F-beta scores.</p> <code>(1)</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MulticlassFbetaScoreResult\n&gt;&gt;&gt; result = MulticlassFbetaScoreResult(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n... )\n&gt;&gt;&gt; result\nMulticlassFbetaScoreResult(y_true=(6,), y_pred=(6,), betas=(1,))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 6,\n 'f1': array([1., 1., 1.]),\n 'macro_f1': 1.0,\n 'micro_f1': 1.0,\n 'weighted_f1': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.MulticlassJaccardResult","title":"arkas.result.MulticlassJaccardResult","text":"<p>               Bases: <code>BaseJaccardResult</code></p> <p>Implement the Jaccard result for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, *)</code> with values in <code>{0, ..., n_classes-1}</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, *)</code> with values in <code>{0, ..., n_classes-1}</code>.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MulticlassJaccardResult\n&gt;&gt;&gt; result = MulticlassJaccardResult(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n... )\n&gt;&gt;&gt; result\nMulticlassJaccardResult(y_true=(6,), y_pred=(6,))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 6,\n 'jaccard': array([1., 1., 1.]),\n 'macro_jaccard': 1.0,\n 'micro_jaccard': 1.0,\n 'weighted_jaccard': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.MulticlassPrecisionResult","title":"arkas.result.MulticlassPrecisionResult","text":"<p>               Bases: <code>BasePrecisionResult</code></p> <p>Implement the precision result for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, *)</code> with values in <code>{0, ..., n_classes-1}</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, *)</code> with values in <code>{0, ..., n_classes-1}</code>.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MulticlassPrecisionResult\n&gt;&gt;&gt; result = MulticlassPrecisionResult(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n... )\n&gt;&gt;&gt; result\nMulticlassPrecisionResult(y_true=(6,), y_pred=(6,))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 6,\n 'macro_precision': 1.0,\n 'micro_precision': 1.0,\n 'precision': array([1., 1., 1.]),\n 'weighted_precision': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.MulticlassRecallResult","title":"arkas.result.MulticlassRecallResult","text":"<p>               Bases: <code>BaseRecallResult</code></p> <p>Implement the recall result for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, *)</code> with values in <code>{0, ..., n_classes-1}</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, *)</code> with values in <code>{0, ..., n_classes-1}</code>.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MulticlassRecallResult\n&gt;&gt;&gt; result = MulticlassRecallResult(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n... )\n&gt;&gt;&gt; result\nMulticlassRecallResult(y_true=(6,), y_pred=(6,))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 6,\n 'macro_recall': 1.0,\n 'micro_recall': 1.0,\n 'recall': array([1., 1., 1.]),\n 'weighted_recall': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.MulticlassRocAucResult","title":"arkas.result.MulticlassRocAucResult","text":"<p>               Bases: <code>BaseRocAucResult</code></p> <p>Implement the Area Under the Receiver Operating Characteristic Curve (ROC AUC) result for multiclass labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MulticlassRocAucResult\n&gt;&gt;&gt; result = MulticlassRocAucResult(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_score=np.array(\n...         [\n...             [0.7, 0.2, 0.1],\n...             [0.4, 0.3, 0.3],\n...             [0.1, 0.8, 0.1],\n...             [0.2, 0.5, 0.3],\n...             [0.3, 0.3, 0.4],\n...             [0.1, 0.2, 0.7],\n...         ]\n...     ),\n... )\n&gt;&gt;&gt; result\nMulticlassRocAucResult(y_true=(6,), y_score=(6, 3))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 6,\n 'macro_roc_auc': 1.0,\n 'micro_roc_auc': 1.0,\n 'roc_auc': array([1., 1., 1.]),\n 'weighted_roc_auc': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.MultilabelAveragePrecisionResult","title":"arkas.result.MultilabelAveragePrecisionResult","text":"<p>               Bases: <code>BaseAveragePrecisionResult</code></p> <p>Implement the average precision result for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MultilabelAveragePrecisionResult\n&gt;&gt;&gt; result = MultilabelAveragePrecisionResult(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_score=np.array([[2, -1, 1], [-1, 1, -2], [0, 2, -3], [3, -2, 4], [1, -3, 5]]),\n... )\n&gt;&gt;&gt; result\nMultilabelAveragePrecisionResult(y_true=(5, 3), y_score=(5, 3))\n&gt;&gt;&gt; result.compute_metrics()\n{'average_precision': array([1., 1., 1.]),\n 'count': 5,\n 'macro_average_precision': 1.0,\n 'micro_average_precision': 1.0,\n 'weighted_average_precision': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.MultilabelConfusionMatrixResult","title":"arkas.result.MultilabelConfusionMatrixResult","text":"<p>               Bases: <code>BaseConfusionMatrixResult</code></p> <p>Implement the confusion matrix result for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, n_classes)</code> with <code>0</code> and <code>1</code> values.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MultilabelConfusionMatrixResult\n&gt;&gt;&gt; result = MultilabelConfusionMatrixResult(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n... )\n&gt;&gt;&gt; result\nMultilabelConfusionMatrixResult(y_true=(5, 3), y_pred=(5, 3))\n&gt;&gt;&gt; result.compute_metrics()\n{'confusion_matrix': array([[[2, 0], [0, 3]],\n                            [[3, 0], [0, 2]],\n                            [[2, 0], [0, 3]]]),\n 'count': 5}\n</code></pre>"},{"location":"refs/result/#arkas.result.MultilabelFbetaScoreResult","title":"arkas.result.MultilabelFbetaScoreResult","text":"<p>               Bases: <code>BaseFbetaScoreResult</code></p> <p>Implement the F-beta result for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, n_classes)</code> with <code>0</code> and <code>1</code> values.</p> required <code>betas</code> <code>Sequence[float]</code> <p>The betas used to compute the F-beta scores.</p> <code>(1)</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MultilabelFbetaScoreResult\n&gt;&gt;&gt; result = MultilabelFbetaScoreResult(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n... )\n&gt;&gt;&gt; result\nMultilabelFbetaScoreResult(y_true=(5, 3), y_pred=(5, 3), betas=(1,))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5,\n 'f1': array([1., 1., 1.]),\n 'macro_f1': 1.0,\n 'micro_f1': 1.0,\n 'weighted_f1': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.MultilabelJaccardResult","title":"arkas.result.MultilabelJaccardResult","text":"<p>               Bases: <code>BaseJaccardResult</code></p> <p>Implement the Jaccard result for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, n_classes)</code> with <code>0</code> and <code>1</code> values.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MultilabelJaccardResult\n&gt;&gt;&gt; result = MultilabelJaccardResult(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n... )\n&gt;&gt;&gt; result\nMultilabelJaccardResult(y_true=(5, 3), y_pred=(5, 3))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5,\n 'jaccard': array([1., 1., 1.]),\n 'macro_jaccard': 1.0,\n 'micro_jaccard': 1.0,\n 'weighted_jaccard': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.MultilabelPrecisionResult","title":"arkas.result.MultilabelPrecisionResult","text":"<p>               Bases: <code>BasePrecisionResult</code></p> <p>Implement the precision result for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, n_classes)</code> with <code>0</code> and <code>1</code> values.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MultilabelPrecisionResult\n&gt;&gt;&gt; result = MultilabelPrecisionResult(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n... )\n&gt;&gt;&gt; result\nMultilabelPrecisionResult(y_true=(5, 3), y_pred=(5, 3))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5,\n 'macro_precision': 1.0,\n 'micro_precision': 1.0,\n 'precision': array([1., 1., 1.]),\n 'weighted_precision': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.MultilabelRecallResult","title":"arkas.result.MultilabelRecallResult","text":"<p>               Bases: <code>BaseRecallResult</code></p> <p>Implement the recall result for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples, n_classes)</code> with <code>0</code> and <code>1</code> values.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MultilabelRecallResult\n&gt;&gt;&gt; result = MultilabelRecallResult(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n... )\n&gt;&gt;&gt; result\nMultilabelRecallResult(y_true=(5, 3), y_pred=(5, 3))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5,\n 'macro_recall': 1.0,\n 'micro_recall': 1.0,\n 'recall': array([1., 1., 1.]),\n 'weighted_recall': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.MultilabelRocAucResult","title":"arkas.result.MultilabelRocAucResult","text":"<p>               Bases: <code>BaseRocAucResult</code></p> <p>Implement the Area Under the Receiver Operating Characteristic Curve (ROC AUC) result for multilabel labels.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples, n_classes)</code> with <code>0</code> and <code>1</code> values.</p> required <code>y_score</code> <code>ndarray</code> <p>The target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. This input must be an array of shape <code>(n_samples, n_classes)</code>.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import MultilabelRocAucResult\n&gt;&gt;&gt; result = MultilabelRocAucResult(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_score=np.array([[2, -1, 1], [-1, 1, -2], [0, 2, -3], [3, -2, 4], [1, -3, 5]]),\n... )\n&gt;&gt;&gt; result\nMultilabelRocAucResult(y_true=(5, 3), y_score=(5, 3))\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5,\n 'macro_roc_auc': 1.0,\n 'micro_roc_auc': 1.0,\n 'roc_auc': array([1., 1., 1.]),\n 'weighted_roc_auc': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.PearsonCorrelationResult","title":"arkas.result.PearsonCorrelationResult","text":"<p>               Bases: <code>BaseResult</code></p> <p>Implement the Pearson correlation result.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>alternative</code> <code>str</code> <p>The alternative hypothesis. Default is 'two-sided'. The following options are available: - 'two-sided': the correlation is nonzero - 'less': the correlation is negative (less than zero) - 'greater': the correlation is positive (greater than zero)</p> <code>'two-sided'</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import PearsonCorrelationResult\n&gt;&gt;&gt; result = PearsonCorrelationResult(\n...     y_true=np.array([1, 2, 3, 4, 5]), y_pred=np.array([1, 2, 3, 4, 5])\n... )\n&gt;&gt;&gt; result\nPearsonCorrelationResult(y_true=(5,), y_pred=(5,), alternative=two-sided)\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5, 'pearson_coeff': 1.0, 'pearson_pvalue': 0.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.PrecisionResult","title":"arkas.result.PrecisionResult","text":"<p>               Bases: <code>BaseResult</code></p> <p>Implement the precision result.</p> <p>This result can be used in 3 different settings:</p> <ul> <li>binary: <code>y_true</code> must be an array of shape <code>(n_samples,)</code>     with <code>0</code> and <code>1</code> values, and <code>y_pred</code> must be an array     of shape <code>(n_samples,)</code>.</li> <li>multiclass: <code>y_true</code> must be an array of shape <code>(n_samples,)</code>     with values in <code>{0, ..., n_classes-1}</code>, and <code>y_pred</code> must     be an array of shape <code>(n_samples,)</code>.</li> <li>multilabel: <code>y_true</code> must be an array of shape     <code>(n_samples, n_classes)</code> with <code>0</code> and <code>1</code> values, and     <code>y_pred</code> must be an array of shape     <code>(n_samples, n_classes)</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import PrecisionResult\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; result = PrecisionResult(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_pred=np.array([1, 0, 0, 1, 1]),\n...     label_type=\"binary\",\n... )\n&gt;&gt;&gt; result\nPrecisionResult(y_true=(5,), y_pred=(5,), label_type=binary)\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5, 'precision': 1.0}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; result = PrecisionResult(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 0], [0, 1, 1], [0, 1, 1], [1, 0, 0], [1, 0, 0]]),\n...     label_type=\"multilabel\",\n... )\n&gt;&gt;&gt; result\nPrecisionResult(y_true=(5, 3), y_pred=(5, 3), label_type=multilabel)\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5,\n 'macro_precision': 0.666...,\n 'micro_precision': 0.714...,\n 'precision': array([1., 1., 0.]),\n 'weighted_precision': 0.625}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; result = PrecisionResult(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n...     label_type=\"multiclass\",\n... )\n&gt;&gt;&gt; result\nPrecisionResult(y_true=(6,), y_pred=(6,), label_type=multiclass)\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 6,\n 'macro_precision': 1.0,\n 'micro_precision': 1.0,\n 'precision': array([1., 1., 1.]),\n 'weighted_precision': 1.0}\n&gt;&gt;&gt; # auto\n&gt;&gt;&gt; result = PrecisionResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n&gt;&gt;&gt; result\nPrecisionResult(y_true=(5,), y_pred=(5,), label_type=binary)\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5, 'precision': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.RecallResult","title":"arkas.result.RecallResult","text":"<p>               Bases: <code>BaseResult</code></p> <p>Implement the recall result.</p> <p>This result can be used in 3 different settings:</p> <ul> <li>binary: <code>y_true</code> must be an array of shape <code>(n_samples,)</code>     with <code>0</code> and <code>1</code> values, and <code>y_pred</code> must be an array     of shape <code>(n_samples,)</code>.</li> <li>multiclass: <code>y_true</code> must be an array of shape <code>(n_samples,)</code>     with values in <code>{0, ..., n_classes-1}</code>, and <code>y_pred</code> must     be an array of shape <code>(n_samples,)</code>.</li> <li>multilabel: <code>y_true</code> must be an array of shape     <code>(n_samples, n_classes)</code> with <code>0</code> and <code>1</code> values, and     <code>y_pred</code> must be an array of shape     <code>(n_samples, n_classes)</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted labels. This input must be an array of shape <code>(n_samples,)</code> or <code>(n_samples, n_classes)</code>.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import RecallResult\n&gt;&gt;&gt; # binary\n&gt;&gt;&gt; result = RecallResult(\n...     y_true=np.array([1, 0, 0, 1, 1]),\n...     y_pred=np.array([1, 0, 0, 1, 1]),\n...     label_type=\"binary\",\n... )\n&gt;&gt;&gt; result\nRecallResult(y_true=(5,), y_pred=(5,), label_type=binary)\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5, 'recall': 1.0}\n&gt;&gt;&gt; # multilabel\n&gt;&gt;&gt; result = RecallResult(\n...     y_true=np.array([[1, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 1], [1, 0, 1]]),\n...     y_pred=np.array([[1, 0, 0], [0, 1, 1], [0, 1, 1], [1, 0, 0], [1, 0, 0]]),\n...     label_type=\"multilabel\",\n... )\n&gt;&gt;&gt; result\nRecallResult(y_true=(5, 3), y_pred=(5, 3), label_type=multilabel)\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5,\n 'macro_recall': 0.666...,\n 'micro_recall': 0.625,\n 'recall': array([1., 1., 0.]),\n 'weighted_recall': 0.625}\n&gt;&gt;&gt; # multiclass\n&gt;&gt;&gt; result = RecallResult(\n...     y_true=np.array([0, 0, 1, 1, 2, 2]),\n...     y_pred=np.array([0, 0, 1, 1, 2, 2]),\n...     label_type=\"multiclass\",\n... )\n&gt;&gt;&gt; result\nRecallResult(y_true=(6,), y_pred=(6,), label_type=multiclass)\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 6,\n 'macro_recall': 1.0,\n 'micro_recall': 1.0,\n 'recall': array([1., 1., 1.]),\n 'weighted_recall': 1.0}\n&gt;&gt;&gt; # auto\n&gt;&gt;&gt; result = RecallResult(\n...     y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n... )\n&gt;&gt;&gt; result\nRecallResult(y_true=(5,), y_pred=(5,), label_type=binary)\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 5, 'recall': 1.0}\n</code></pre>"},{"location":"refs/result/#arkas.result.Result","title":"arkas.result.Result","text":"<p>               Bases: <code>BaseResult</code></p> <p>Implement a simple result.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>dict | None</code> <p>The metrics.</p> <code>None</code> <code>figures</code> <code>dict | None</code> <p>The figures.</p> <code>None</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.result import Result\n&gt;&gt;&gt; result = Result(metrics={\"accuracy\": 1.0, \"count\": 42}, figures={})\n&gt;&gt;&gt; result\nResult(metrics=2, figures=0)\n&gt;&gt;&gt; result.compute_metrics()\n{'accuracy': 1.0, 'count': 42}\n</code></pre>"},{"location":"refs/result/#arkas.result.SequentialResult","title":"arkas.result.SequentialResult","text":"<p>               Bases: <code>BaseResult</code></p> <p>Implement a result to merge multiple result objects into a single result object.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>Sequence[BaseResult]</code> <p>The results to merge. This order is used to merge the metrics and figures if they have duplicate keys, i.e. only the last value for each key is kept.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import SequentialResult, Result\n&gt;&gt;&gt; result = SequentialResult(\n...     [\n...         Result(metrics={\"accuracy\": 62.0, \"count\": 42}),\n...         Result(metrics={\"ap\": 0.42, \"count\": 42}),\n...     ]\n... )\n&gt;&gt;&gt; result\nSequentialResult(count=2)\n&gt;&gt;&gt; result.compute_metrics()\n{'accuracy': 62.0, 'count': 42, 'ap': 0.42}\n</code></pre>"},{"location":"refs/result/#arkas.result.SpearmanCorrelationResult","title":"arkas.result.SpearmanCorrelationResult","text":"<p>               Bases: <code>BaseResult</code></p> <p>Implement the Spearman correlation result.</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>ndarray</code> <p>The ground truth target values.</p> required <code>y_pred</code> <code>ndarray</code> <p>The predicted values.</p> required <code>alternative</code> <code>str</code> <p>The alternative hypothesis. Default is 'two-sided'. The following options are available: - 'two-sided': the correlation is nonzero - 'less': the correlation is negative (less than zero) - 'greater': the correlation is positive (greater than zero)</p> <code>'two-sided'</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.result import SpearmanCorrelationResult\n&gt;&gt;&gt; result = SpearmanCorrelationResult(\n...     y_true=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n...     y_pred=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n... )\n&gt;&gt;&gt; result\nSpearmanCorrelationResult(y_true=(9,), y_pred=(9,), alternative=two-sided)\n&gt;&gt;&gt; result.compute_metrics()\n{'count': 9, 'spearman_coeff': 1.0, 'spearman_pvalue': 0.0}\n</code></pre>"},{"location":"refs/runner/","title":"arkas.runner","text":""},{"location":"refs/runner/#arkas.runner","title":"arkas.runner","text":"<p>Contain runners.</p>"},{"location":"refs/runner/#arkas.runner.BaseRunner","title":"arkas.runner.BaseRunner","text":"<p>               Bases: <code>ABC</code></p> <p>Define the base class to implement a runner.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from iden.io import PickleSaver\n&gt;&gt;&gt; from grizz.ingestor import Ingestor\n&gt;&gt;&gt; from arkas.evaluator import AccuracyEvaluator\n&gt;&gt;&gt; from arkas.runner import EvaluationRunner\n&gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n...     path = Path(tmpdir).joinpath(\"metrics.pkl\")\n...     runner = EvaluationRunner(\n...         ingestor=Ingestor(\n...             pl.DataFrame(\n...                 {\n...                     \"pred\": [3, 2, 0, 1, 0],\n...                     \"target\": [3, 2, 0, 1, 0],\n...                 }\n...             )\n...         ),\n...         evaluator=AccuracyEvaluator(y_true=\"target\", y_pred=\"pred\"),\n...         saver=PickleSaver(),\n...         path=path,\n...     )\n...     print(runner)\n...     runner.run()\n...\nEvaluationRunner(\n  (ingestor): Ingestor(shape=(5, 2))\n  (evaluator): AccuracyEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n  (saver): PickleSaver(protocol=5)\n  (path): .../metrics.pkl\n  (show_metrics): True\n)\n</code></pre>"},{"location":"refs/runner/#arkas.runner.BaseRunner.run","title":"arkas.runner.BaseRunner.run  <code>abstractmethod</code>","text":"<pre><code>run() -&gt; Any\n</code></pre> <p>Execute the logic of the runner.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Any artifact of the runner</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from iden.io import PickleSaver\n&gt;&gt;&gt; from grizz.ingestor import Ingestor\n&gt;&gt;&gt; from arkas.evaluator import AccuracyEvaluator\n&gt;&gt;&gt; from arkas.runner import EvaluationRunner\n&gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n...     path = Path(tmpdir).joinpath(\"metrics.pkl\")\n...     runner = EvaluationRunner(\n...         ingestor=Ingestor(\n...             pl.DataFrame(\n...                 {\n...                     \"pred\": [3, 2, 0, 1, 0],\n...                     \"target\": [3, 2, 0, 1, 0],\n...                 }\n...             )\n...         ),\n...         evaluator=AccuracyEvaluator(y_true=\"target\", y_pred=\"pred\"),\n...         saver=PickleSaver(),\n...         path=path,\n...     )\n...     runner.run()\n...\n</code></pre>"},{"location":"refs/runner/#arkas.runner.EvaluationRunner","title":"arkas.runner.EvaluationRunner","text":"<p>               Bases: <code>BaseRunner</code></p> <p>Implement a simple evaluation runner.</p> <p>Parameters:</p> Name Type Description Default <code>ingestor</code> <code>BaseIngestor | dict</code> <p>The data ingestor or its configuration.</p> required <code>evaluator</code> <code>BaseEvaluator | dict</code> <p>The evaluator or its configuration.</p> required <code>saver</code> <code>BaseSaver | dict</code> <p>The metric saver or its configuration.</p> required <code>path</code> <code>Path | str</code> <p>The path where to save the metrics.</p> required <code>show_metrics</code> <code>bool</code> <p>If <code>True</code>, the metrics are shown in the logging output.</p> <code>True</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from iden.io import PickleSaver\n&gt;&gt;&gt; from grizz.ingestor import Ingestor\n&gt;&gt;&gt; from arkas.evaluator import AccuracyEvaluator\n&gt;&gt;&gt; from arkas.runner import EvaluationRunner\n&gt;&gt;&gt; with tempfile.TemporaryDirectory() as tmpdir:\n...     path = Path(tmpdir).joinpath(\"metrics.pkl\")\n...     runner = EvaluationRunner(\n...         ingestor=Ingestor(\n...             pl.DataFrame(\n...                 {\n...                     \"pred\": [3, 2, 0, 1, 0],\n...                     \"target\": [3, 2, 0, 1, 0],\n...                 }\n...             )\n...         ),\n...         evaluator=AccuracyEvaluator(y_true=\"target\", y_pred=\"pred\"),\n...         saver=PickleSaver(),\n...         path=path,\n...     )\n...     print(runner)\n...     runner.run()\n...\nEvaluationRunner(\n  (ingestor): Ingestor(shape=(5, 2))\n  (evaluator): AccuracyEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n  (saver): PickleSaver(protocol=5)\n  (path): .../metrics.pkl\n  (show_metrics): True\n)\n</code></pre>"},{"location":"refs/runner/#arkas.runner.is_runner_config","title":"arkas.runner.is_runner_config","text":"<pre><code>is_runner_config(config: dict) -&gt; bool\n</code></pre> <p>Indicate if the input configuration is a configuration for a <code>BaseRunner</code>.</p> <p>This function only checks if the value of the key  <code>_target_</code> is valid. It does not check the other values. If <code>_target_</code> indicates a function, the returned type hint is used to check the class.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>The configuration to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the input configuration is a configuration for a <code>BaseRunner</code> object.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.runner import is_runner_config\n&gt;&gt;&gt; is_runner_config({\"_target_\": \"arkas.runner.EvaluationRunner\"})\nTrue\n</code></pre>"},{"location":"refs/runner/#arkas.runner.setup_runner","title":"arkas.runner.setup_runner","text":"<pre><code>setup_runner(runner: BaseRunner | dict) -&gt; BaseRunner\n</code></pre> <p>Set up a runner.</p> <p>The runner is instantiated from its configuration by using the <code>BaseRunner</code> factory function.</p> <p>Parameters:</p> Name Type Description Default <code>runner</code> <code>BaseRunner | dict</code> <p>Specifies a runner or its configuration.</p> required <p>Returns:</p> Type Description <code>BaseRunner</code> <p>An instantiated runner.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from arkas.runner import setup_runner\n&gt;&gt;&gt; runner = setup_runner(\n...     {\n...         \"_target_\": \"arkas.runner.EvaluationRunner\",\n...         \"ingestor\": {\n...             \"_target_\": \"grizz.ingestor.Ingestor\",\n...             \"frame\": pl.DataFrame(\n...                 {\n...                     \"pred\": [3, 2, 0, 1, 0],\n...                     \"target\": [3, 2, 0, 1, 0],\n...                 }\n...             ),\n...         },\n...         \"evaluator\": {\n...             \"_target_\": \"arkas.evaluator.AccuracyEvaluator\",\n...             \"y_true\": \"target\",\n...             \"y_pred\": \"pred\",\n...         },\n...         \"saver\": {\"_target_\": \"iden.io.PickleSaver\"},\n...         \"path\": \"/tmp/data/metrics.pkl\",\n...     }\n... )\n&gt;&gt;&gt; runner\nEvaluationRunner(\n  (ingestor): Ingestor(shape=(5, 2))\n  (evaluator): AccuracyEvaluator(y_true=target, y_pred=pred, drop_nulls=True)\n  (saver): PickleSaver(protocol=5)\n  (path): .../metrics.pkl\n  (show_metrics): True\n)\n</code></pre>"},{"location":"refs/section/","title":"arkas.section","text":""},{"location":"refs/section/#arkas.section","title":"arkas.section","text":"<p>Contain sections.</p>"},{"location":"refs/section/#arkas.section.AccuracySection","title":"arkas.section.AccuracySection","text":"<p>               Bases: <code>BaseSection</code></p> <p>Implement a section that analyze accuracy results.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>BaseResult</code> <p>The data structure containing the results.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.section import AccuracySection\n&gt;&gt;&gt; from arkas.result import AccuracyResult\n&gt;&gt;&gt; section = AccuracySection(\n...     result=AccuracyResult(\n...         y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n...     )\n... )\n&gt;&gt;&gt; section\nAccuracySection(\n  (result): AccuracyResult(y_true=(5,), y_pred=(5,))\n)\n&gt;&gt;&gt; section.generate_html_body()\n</code></pre>"},{"location":"refs/section/#arkas.section.BaseSection","title":"arkas.section.BaseSection","text":"<p>               Bases: <code>ABC</code></p> <p>Define the base class to manage sections.</p>"},{"location":"refs/section/#arkas.section.BaseSection.equal","title":"arkas.section.BaseSection.equal  <code>abstractmethod</code>","text":"<pre><code>equal(other: Any) -&gt; bool\n</code></pre> <p>Indicate if two sections are equal or not.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Any</code> <p>The other section to compare.</p> required <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the two sections are equal, otherwise <code>False</code>.</p>"},{"location":"refs/section/#arkas.section.BaseSection.generate_html_body","title":"arkas.section.BaseSection.generate_html_body  <code>abstractmethod</code>","text":"<pre><code>generate_html_body(\n    number: str = \"\",\n    tags: Sequence[str] = (),\n    depth: int = 0,\n) -&gt; str\n</code></pre> <p>Return the HTML body associated to the section.</p> <p>Parameters:</p> Name Type Description Default <code>number</code> <code>str</code> <p>The section number.</p> <code>''</code> <code>tags</code> <code>Sequence[str]</code> <p>The tags associated to the section.</p> <code>()</code> <code>depth</code> <code>int</code> <p>The depth in the report.</p> <code>0</code> <p>Returns:</p> Type Description <code>str</code> <p>The HTML body associated to the section.</p>"},{"location":"refs/section/#arkas.section.BaseSection.generate_html_toc","title":"arkas.section.BaseSection.generate_html_toc  <code>abstractmethod</code>","text":"<pre><code>generate_html_toc(\n    number: str = \"\",\n    tags: Sequence[str] = (),\n    depth: int = 0,\n    max_depth: int = 1,\n) -&gt; str\n</code></pre> <p>Return the HTML table of content (TOC) associated to the section.</p> <p>Parameters:</p> Name Type Description Default <code>number</code> <code>str</code> <p>The section number associated to the section.</p> <code>''</code> <code>tags</code> <code>Sequence[str]</code> <p>The tags associated to the section.</p> <code>()</code> <code>depth</code> <code>int</code> <p>The depth in the report.</p> <code>0</code> <code>max_depth</code> <code>int</code> <p>The maximum depth to generate in the TOC.</p> <code>1</code> <p>Returns:</p> Type Description <code>str</code> <p>The HTML table of content associated to the section.</p>"},{"location":"refs/section/#arkas.section.BinaryPrecisionSection","title":"arkas.section.BinaryPrecisionSection","text":"<p>               Bases: <code>BaseSection</code></p> <p>Implement a section that analyze precision results.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>BaseResult</code> <p>The data structure containing the results.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from arkas.section import BinaryPrecisionSection\n&gt;&gt;&gt; from arkas.result import BinaryPrecisionResult\n&gt;&gt;&gt; section = BinaryPrecisionSection(\n...     result=BinaryPrecisionResult(\n...         y_true=np.array([1, 0, 0, 1, 1]), y_pred=np.array([1, 0, 0, 1, 1])\n...     )\n... )\n&gt;&gt;&gt; section\nBinaryPrecisionSection(\n  (result): BinaryPrecisionResult(y_true=(5,), y_pred=(5,))\n)\n&gt;&gt;&gt; section.generate_html_body()\n</code></pre>"},{"location":"refs/section/#arkas.section.ContentSection","title":"arkas.section.ContentSection","text":"<p>               Bases: <code>BaseSection</code></p> <p>Implement a section that generates the given custom content.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content to use in the HTML code.</p> required <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.section import ContentSection\n&gt;&gt;&gt; section = ContentSection(content=\"meow\")\n&gt;&gt;&gt; section\nContentSection()\n&gt;&gt;&gt; section.generate_html_body()\n</code></pre>"},{"location":"refs/utils/","title":"arkas.utils","text":""},{"location":"refs/utils/#arkas.utils","title":"arkas.utils","text":"<p>Contain utility functions.</p>"},{"location":"refs/utils/#arkas.utils.factory","title":"arkas.utils.factory","text":"<p>Contain a function to instantiate an object from its configuration.</p>"},{"location":"refs/utils/#arkas.utils.factory.setup_object","title":"arkas.utils.factory.setup_object","text":"<pre><code>setup_object(obj_or_config: T | dict) -&gt; T\n</code></pre> <p>Set up an object from its configuration.</p> <p>Parameters:</p> Name Type Description Default <code>obj_or_config</code> <code>T | dict</code> <p>The object or its configuration.</p> required <p>Returns:</p> Type Description <code>T</code> <p>The instantiated object.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.factory import setup_object\n&gt;&gt;&gt; obj = setup_object({\"_target_\": \"collections.deque\", \"iterable\": [1, 2, 1, 3]})\n&gt;&gt;&gt; obj\ndeque([1, 2, 1, 3])\n&gt;&gt;&gt; setup_object(obj)  # Do nothing because the object is already instantiated\ndeque([1, 2, 1, 3])\n</code></pre>"},{"location":"refs/utils/#arkas.utils.figure","title":"arkas.utils.figure","text":"<p>Contain utility functions to manage matplotlib figures.</p>"},{"location":"refs/utils/#arkas.utils.figure.figure2html","title":"arkas.utils.figure.figure2html","text":"<pre><code>figure2html(\n    fig: Figure | None,\n    reactive: bool = True,\n    close_fig: bool = False,\n) -&gt; str\n</code></pre> <p>Convert a matplotlib figure to a string that can be used in a HTML file.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure | None</code> <p>The figure to convert.</p> required <code>reactive</code> <code>bool</code> <p>If <code>True</code>, the generated is configured to be reactive to the screen size.</p> <code>True</code> <code>close_fig</code> <code>bool</code> <p>If <code>True</code>, the figure is closed after it is converted to HTML format.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The converted figure to a string.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from matplotlib import pyplot as plt\n&gt;&gt;&gt; from arkas.utils.figure import figure2html\n&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; string = figure2html(fig)\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports","title":"arkas.utils.imports","text":"<p>Implement some utility functions to manage optional dependencies.</p>"},{"location":"refs/utils/#arkas.utils.imports.check_colorlog","title":"arkas.utils.imports.check_colorlog","text":"<pre><code>check_colorlog() -&gt; None\n</code></pre> <p>Check if the <code>colorlog</code> package is installed.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the <code>colorlog</code> package is not installed.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import check_colorlog\n&gt;&gt;&gt; check_colorlog()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.check_hya","title":"arkas.utils.imports.check_hya","text":"<pre><code>check_hya() -&gt; None\n</code></pre> <p>Check if the <code>hya</code> package is installed.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the <code>hya</code> package is not installed.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import check_hya\n&gt;&gt;&gt; check_hya()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.check_hydra","title":"arkas.utils.imports.check_hydra","text":"<pre><code>check_hydra() -&gt; None\n</code></pre> <p>Check if the <code>hydra</code> package is installed.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the <code>hydra</code> package is not installed.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import check_hydra\n&gt;&gt;&gt; check_hydra()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.check_markdown","title":"arkas.utils.imports.check_markdown","text":"<pre><code>check_markdown() -&gt; None\n</code></pre> <p>Check if the <code>markdown</code> package is installed.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the <code>markdown</code> package is not installed.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import check_markdown\n&gt;&gt;&gt; check_markdown()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.check_omegaconf","title":"arkas.utils.imports.check_omegaconf","text":"<pre><code>check_omegaconf() -&gt; None\n</code></pre> <p>Check if the <code>omegaconf</code> package is installed.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the <code>omegaconf</code> package is not installed.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import check_omegaconf\n&gt;&gt;&gt; check_omegaconf()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.check_scipy","title":"arkas.utils.imports.check_scipy","text":"<pre><code>check_scipy() -&gt; None\n</code></pre> <p>Check if the <code>scipy</code> package is installed.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the <code>scipy</code> package is not installed.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import check_scipy\n&gt;&gt;&gt; check_scipy()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.colorlog_available","title":"arkas.utils.imports.colorlog_available","text":"<pre><code>colorlog_available(\n    fn: Callable[..., Any]\n) -&gt; Callable[..., Any]\n</code></pre> <p>Implement a decorator to execute a function only if <code>colorlog</code> package is installed.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., Any]</code> <p>The function to execute.</p> required <p>Returns:</p> Type Description <code>Callable[..., Any]</code> <p>A wrapper around <code>fn</code> if <code>colorlog</code> package is installed, otherwise <code>None</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import colorlog_available\n&gt;&gt;&gt; @colorlog_available\n... def my_function(n: int = 0) -&gt; int:\n...     return 42 + n\n...\n&gt;&gt;&gt; my_function()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.hya_available","title":"arkas.utils.imports.hya_available","text":"<pre><code>hya_available(fn: Callable[..., Any]) -&gt; Callable[..., Any]\n</code></pre> <p>Implement a decorator to execute a function only if <code>hya</code> package is installed.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., Any]</code> <p>The function to execute.</p> required <p>Returns:</p> Type Description <code>Callable[..., Any]</code> <p>A wrapper around <code>fn</code> if <code>hya</code> package is installed, otherwise <code>None</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import hya_available\n&gt;&gt;&gt; @hya_available\n... def my_function(n: int = 0) -&gt; int:\n...     return 42 + n\n...\n&gt;&gt;&gt; my_function()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.hydra_available","title":"arkas.utils.imports.hydra_available","text":"<pre><code>hydra_available(\n    fn: Callable[..., Any]\n) -&gt; Callable[..., Any]\n</code></pre> <p>Implement a decorator to execute a function only if <code>hydra</code> package is installed.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., Any]</code> <p>The function to execute.</p> required <p>Returns:</p> Type Description <code>Callable[..., Any]</code> <p>A wrapper around <code>fn</code> if <code>hydra</code> package is installed, otherwise <code>None</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import hydra_available\n&gt;&gt;&gt; @hydra_available\n... def my_function(n: int = 0) -&gt; int:\n...     return 42 + n\n...\n&gt;&gt;&gt; my_function()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.is_colorlog_available","title":"arkas.utils.imports.is_colorlog_available","text":"<pre><code>is_colorlog_available() -&gt; bool\n</code></pre> <p>Indicate if the <code>colorlog</code> package is installed or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if <code>colorlog</code> is available otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import is_colorlog_available\n&gt;&gt;&gt; is_colorlog_available()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.is_hya_available","title":"arkas.utils.imports.is_hya_available","text":"<pre><code>is_hya_available() -&gt; bool\n</code></pre> <p>Indicate if the <code>hya</code> package is installed or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if <code>hya</code> is available otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import is_hya_available\n&gt;&gt;&gt; is_hya_available()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.is_hydra_available","title":"arkas.utils.imports.is_hydra_available","text":"<pre><code>is_hydra_available() -&gt; bool\n</code></pre> <p>Indicate if the <code>hydra</code> package is installed or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if <code>hydra</code> is available otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import is_hydra_available\n&gt;&gt;&gt; is_hydra_available()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.is_markdown_available","title":"arkas.utils.imports.is_markdown_available","text":"<pre><code>is_markdown_available() -&gt; bool\n</code></pre> <p>Indicate if the <code>markdown</code> package is installed or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if <code>markdown</code> is available otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import is_markdown_available\n&gt;&gt;&gt; is_markdown_available()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.is_omegaconf_available","title":"arkas.utils.imports.is_omegaconf_available","text":"<pre><code>is_omegaconf_available() -&gt; bool\n</code></pre> <p>Indicate if the <code>omegaconf</code> package is installed or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if <code>omegaconf</code> is available otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import is_omegaconf_available\n&gt;&gt;&gt; is_omegaconf_available()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.is_scipy_available","title":"arkas.utils.imports.is_scipy_available","text":"<pre><code>is_scipy_available() -&gt; bool\n</code></pre> <p>Indicate if the <code>scipy</code> package is installed or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if <code>scipy</code> is available otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import is_scipy_available\n&gt;&gt;&gt; is_scipy_available()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.markdown_available","title":"arkas.utils.imports.markdown_available","text":"<pre><code>markdown_available(\n    fn: Callable[..., Any]\n) -&gt; Callable[..., Any]\n</code></pre> <p>Implement a decorator to execute a function only if <code>markdown</code> package is installed.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., Any]</code> <p>The function to execute.</p> required <p>Returns:</p> Type Description <code>Callable[..., Any]</code> <p>A wrapper around <code>fn</code> if <code>markdown</code> package is installed, otherwise <code>None</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import markdown_available\n&gt;&gt;&gt; @markdown_available\n... def my_function(n: int = 0) -&gt; int:\n...     return 42 + n\n...\n&gt;&gt;&gt; my_function()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.omegaconf_available","title":"arkas.utils.imports.omegaconf_available","text":"<pre><code>omegaconf_available(\n    fn: Callable[..., Any]\n) -&gt; Callable[..., Any]\n</code></pre> <p>Implement a decorator to execute a function only if <code>omegaconf</code> package is installed.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., Any]</code> <p>The function to execute.</p> required <p>Returns:</p> Type Description <code>Callable[..., Any]</code> <p>A wrapper around <code>fn</code> if <code>omegaconf</code> package is installed, otherwise <code>None</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import omegaconf_available\n&gt;&gt;&gt; @omegaconf_available\n... def my_function(n: int = 0) -&gt; int:\n...     return 42 + n\n...\n&gt;&gt;&gt; my_function()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.imports.scipy_available","title":"arkas.utils.imports.scipy_available","text":"<pre><code>scipy_available(\n    fn: Callable[..., Any]\n) -&gt; Callable[..., Any]\n</code></pre> <p>Implement a decorator to execute a function only if <code>scipy</code> package is installed.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., Any]</code> <p>The function to execute.</p> required <p>Returns:</p> Type Description <code>Callable[..., Any]</code> <p>A wrapper around <code>fn</code> if <code>scipy</code> package is installed, otherwise <code>None</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.imports import scipy_available\n&gt;&gt;&gt; @scipy_available\n... def my_function(n: int = 0) -&gt; int:\n...     return 42 + n\n...\n&gt;&gt;&gt; my_function()\n</code></pre>"},{"location":"refs/utils/#arkas.utils.text","title":"arkas.utils.text","text":"<p>Contain text utility functions.</p>"},{"location":"refs/utils/#arkas.utils.text.markdown_to_html","title":"arkas.utils.text.markdown_to_html","text":"<pre><code>markdown_to_html(\n    text: str, ignore_error: bool = False\n) -&gt; str\n</code></pre> <p>Convert a markdown text to HTML text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The markdown text to convert.</p> required <code>ignore_error</code> <code>bool</code> <p>If <code>False</code>, an error is raised if <code>markdown</code> is not installed, otherwise the input text is returned.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The converted text if <code>markdown</code> is installed, otherwise the input text.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from arkas.utils.text import markdown_to_html\n&gt;&gt;&gt; out = markdown_to_html(\"- a\\n- b\\n- c\")\n&gt;&gt;&gt; print(out)\n&lt;ul&gt;\n&lt;li&gt;a&lt;/li&gt;\n&lt;li&gt;b&lt;/li&gt;\n&lt;li&gt;c&lt;/li&gt;\n&lt;/ul&gt;\n</code></pre>"}]}